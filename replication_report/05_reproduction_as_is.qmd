---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Reproduction "as is" of the publised results

We want to make all code modifications transparent, so we download a clean version of Wolf et al. source code.

```{r}
# URL pointing to the zip download of the Wolf et al. repository
repo <- "https://codeload.github.com/wolfch2/PA_matching/zip/refs/heads/master"

# downloads locally
download.file(repo, destfile = "original.zip", method = "curl")
# unzips to a containing folder named PA_matching-master (github default)
unzip("original.zip")
# original folder will remain intact, we make a copy with required edits to run
dir.create("PA_matching_asis")
file.copy(list.files("PA_matching-master", full.names = TRUE), 
          "PA_matching_asis")
```

## Original source code execution

Some code modifications are required for the code to run. These edits are not intended to test the robustness of the analysis, just for the successful run of the scripts.

We will use the following function that explicitly declares which modification are made in the source code.

```{r}
library(tidyverse)

# Not a good practice, only for debugging
# setwd("replication_report")

# A function to replace some parts of the code
replace_all <- function(x, pattern, replacement) {
  print(x)
  readLines(x) %>%
    str_replace_all(pattern, replacement) %>%
    writeLines(x)
}
```

The following functions are a series of utilities to save the data to a datalake (based on S3)

```{r}
library(aws.s3)

# Gather secrets required to work with S3 systems
if (Sys.info()["sysname"] == "Windows") {
  if (file.exists("secrets_aws.R")) {
    source("secrets_aws.R")
  } else { 
    print(paste0(
    "You must save your AWS credentials in a local file named secret_aws.R. ",
    "For more indications see: https://www.book.utilitr.org/",
    "sspcloud.html#renouveler-des-droits-dacc%C3%A8s-p%C3%A9rim%C3%A9s"))
  }
}

# If running on SSP Cloud, getting the key from vault
if (Sys.info()["effective_user"] == "onyxia") {
  system("vault kv get -format=table onyxia-kv/fbedecarrats/species_info", 
         intern = TRUE) -> my_secret 
  my_key_zip <- my_secret %>%
    pluck(18) %>%
    str_extract("[:alnum:]*$")
  my_key_api <- my_secret %>%
    pluck(17) %>%
    str_extract("[:alnum:]*$")
} else {
 my_secret <- readLines("secret_zip_key")
}

# A function to put data from local machine to S3
put_to_s3 <- function(from, to) {
  aws.s3::put_object(
    file = from,
    object = to,
    bucket = "fbedecarrats",
    region = "",
    multipart = TRUE)
}

# A function to iterate/vectorize copy
save_from_s3 <- function(from, to) {
  aws.s3::save_object(
    object = from,
    bucket = "fbedecarrats",
    file = to,
    overwrite = FALSE,
    region = "")
}

# Listing files in bucket
my_files <- get_bucket_df(bucket = "fbedecarrats",
                          prefix = "Replication_wolf",
                          region = "")
```


### Fetch data from Google Earth Engine

Mofify the paths

```{r}
GEE_data_script <- "PA_matching_asis/001 - dl_GEE.py"
my_data_input <- paste0(getwd(), "/data_input/")
replace_all(GEE_data_script, 
            "/home/chrisgraywolf/shared/analysis/PA_matching/data_input/",
            my_data_input)
```

Do-not re-execute this code: fetch existing data instead.

```{r}
#| eval: false
GEE_data_script <- "PA_matching_asis/001 - dl_GEE.py"
source_python(GEE_data_script)
```

The code below saves the output data to a private data lake.


```{r}
gee_pattern <- "(land|elev|loss|gain|cover|mask|travel_time|pop_dens).*tif"
# List the outputs of the GEE data fetching
gee <- list.files(path = "data_input", full.names = TRUE,
                  pattern = paste0("^", gee_pattern))
# Transform to specify their location on S3
gee_s3 <- str_replace(s3, "data_input", "Replication_wolf")
# Send to S3
map2(gee, gee_s3 , put_to_s3)
```

The code below fetches the output data from a private data lake.

```{r}
# naming pattern of files fetched from Google Earth Engine
gee_pattern <- "(land|elev|loss|gain|cover|mask|travel_time|pop_dens).*tif"
# Filter those files from bucket content
gee_s3 <- my_files %>%
  filter(str_detect(Key, paste0("Replication_wolf/", gee_pattern))) %>%
  pluck("Key")
# rename for local location
gee_local <- str_replace(gee_s3, "Replication_wolf", "data_input")
# Copy locally
map2(gee_s3, gee_local, save_from_s3)
```


### Fetch species data from IUCN

Modify the script to enable it to run on another machine.

The column names in the official BOTW database are now in lowercase. It was apparently in uppercase when Wolf et al. processed the data. We need to modify the reference to one column name in the script for it to run. 

```{r}
IUCN_data_script <- "PA_matching_asis/002 - dl_IUCN.R"
my_wd <- paste0(getwd(), "/")

replace_all(IUCN_data_script, 
            "/home/chrisgraywolf/analysis_desktop/PA_matching/",
            my_wd)
replace_all(IUCN_data_script, 
            "put_your_token_here",
            my_key_api)
# Column name to lowercase
replace_all(IUCN_data_script, "SISID", "sisid")
```

Run the script

```{r}
source(IUCN_data_script)
```

Save the output data

```{r}
# Species data csv (to avoid re-querying IUCN API at each re-run)
put_to_s3(from = "data_processed/species_data.csv", 
          to = "Replication_wolf/data_processed/species_data.csv")
bird_shps <- list.files(path = "data_input/range_maps",
                        pattern = "BIRDS_.*", full.names = TRUE)
system(paste0("zip ", paste(bird_shps, collapse = " "), " -p ", my_key_zip, 
              "birds_shp.zip"))

# The main output of the script.
put_to_s3(from = "data_processed/rasters/elev.tif", 
          to = "Replication_wolf/data_processed/rasters/elev.tif")
```

Load the data 

```{r}
save_from_s3(from = "Replication_wolf/data_processed/species_data.csv",
             to = "data_processed/species_data.csv")
# species_data <- read_csv("data_processed/species_data.csv")
save_from_s3(from = "Replication_wolf/data_processed/rasters/elev.tif",
             to = "data_processed/rasters/elev.tif")
```

### Compute species richness

```{r}
system(paste("unzip -P", my_key, "other.zip"))
file.remove("other.zip")
```


### Join rasters

This steps corresponds to the file `004 - join_rasters.R`, which original content can be examined below.

::: {.callout-code collapse="true"}
```{r}
#| echo: false
#| warning: false
#| class-output: "sourceCode python"

cat(readLines("PA_matching_asis/004 - join_rasters.R"), sep = "\n")
```
:::

This script cannot be run without modification. The location of the working directory was specific to the author machine, we replace by a generic location. The script fetches data from Curtis et al \[INSERT CITATION\], but the URL has changed. We replace with the new URL and modify too the file name with is now all in lowercase.
The URLs for Curtis et al. data and WWF data have changed
There is a typo on line 136: an object called `PA_dists` is called but no object exists with this name, however an object `PAs_dist` (without a final "s") was created on the previous code line


```{r}
join_script <- "PA_matching_asis/004 - join_rasters.R"
# Change the working directory location
replace_all(join_script, # in the original script
            "\"/home/chrisgraywolf/shared/analysis/PA_matching/\"", 
            "paste0(getwd(), \"/\")") # keep the current working directory instead

replace_all(join_script, # Replace Science URL that has changed
            "https://science.sciencemag.org/highwire/filestream/715492/field_highwire_adjunct_files/2/aau3445-Data-S3.tif", 
             "https://www.science.org/action/downloadSupplement?doi=10.1126%2Fscience.aau3445&file=aau3445-data-s3.tif")
# Replace Curtis et al filename: lowercased by Science portal admins
replace_all(join_script, 
            "data_input/aau3445-Data-S3.tif",
            "data_input/aau3445-data-s3.tif")
# We have not found a way to fetch a previous version of the WDPA database
# January 2023 is the one we use.
replace_all(join_script, 
            "WDPA_Jan2020",
            "WDPA_Jan2023")
# Replace url to download WWF data
replace_all(join_script, 
            "https://c402277.ssl.cf1.rackcdn.com/publications/15/files/original/official_teow.zip?1349272619",
            "https://files.worldwildlife.org/wwfcmsprod/files/Publication/file/6kcchn7e3u_official_teow.zip")
# Correct the typo in the code
replace_all(join_script, 
            "PAs_buffer = PA_dists <= 10e3",
            "PAs_buffer = PAs_dist <= 10e3")
```

Execute the code

```{r}
source(join_script)
```

### Match pixels on covariates


```{r}
matching_script <- "PA_matching_asis/005 - covariate_matching.jl"

my_wd <- paste0(getwd(), "/")

# Change the working directory location
replace_all(matching_script, # in the original script
            "\"/home/chrisgraywolf/shared/analysis/PA_matching/\"", 
            my_wd) # keep the current working directory instead
```



```{r}
#| eval: true
library(JuliaCall)
julia_source(matching_script)
```


### Merge the results

### Perform country level analysis

### Produce summary figures

### Compute SVC model

### Run matching sensitivity
