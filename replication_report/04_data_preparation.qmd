---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Data gathering and preparation

## General setup

A challenge for this replication 
The analysis mentions that the sources are publicly available, but the data is not available as is: needs an access to Google Earth Engine. The API is not public and this accessibility is conditioned by the will of Google to keep its platform in free access and not to make any evolution in the GEE API that would not be backward compatible. For these resons, it seems recommended to archive the data outputed by sources which reproducibility is not guaranteed.

### Chose mode: `iterative-development` or `from-scratch`

We propose two modes:

-   `from-scratch` starts from a clean slates and gathers all the data from the original sources

-   `iterative-development` gathers the data produced by previous steps that has been provisionnally stored in a data lake

Some data (such as the protected areas or the ) so they are encrypted and stored in private location accessible only to the replication team. Others who might want to reproduce the analysis must necessarily turn the mode to `from-scratch`.

```{r select_mode}
# choose between "iterative-development" and "from stratch
data_mode <- "iterative-development" 

# boolean values from the previous
iterative_development_mode <- data_mode == "iterative-development"
from_scratch_mode <- data_mode == "from-scratch"
```

### Add required subfolders

```{r create_dirs}
dir.create("data_input")
dir.create("data_input/PAs")
dir.create("temp")
dir.create("data_processed")
dir.create("data_processed/rasters")
```


## Fetch source data

We downloaded the data from protectedplanet.net and placed it in the folder `replication_report/temp`. We then executed the following script to have it in the right format.

get the right WDPA data. Wolf et al. use the data from January 2020.

```{r}
library(aws.s3)
save_from_s3(from = "Replication_wolf/WDPA_Jan2023_Public.gdb.zip",
             to = "data_input/WDPA_Jan2023_Public.gdb.zip")
```
```{r fetch_wdpa}
#| eval: !expr from_scratch_mode

library(tidyverse)
library(wdpar)
library(sf)

# Get the data from protectedplanet
if (file.exists("data_input/WDPA_Jan2023_Public.gdb.zip")) {
  wdpa_global <- wdpa_read("data_input/WDPA_Jan2023_Public.gdb.zip")
} else {
  wdpa_global <- wdpa_fetch("global", download_dir = "data_input")
}

# Build a name with same structure than Wolf et al.
wdpa_file_name <- list.files("data_input", pattern = ".gdb.zip") %>%
  str_remove("_Public.gdb.zip") %>%
  paste0("data_input/PAs/", ., "-shapefile-polygons.shp")
# Filter for polygons (filtering out points to be sure not to exclude something
# by mistake) and write locally.
wdpa_global %>%
  #"We excluded PAs from our primary analysis that: 
  #[1] had point (centroid) information only, 
  filter(st_geometry_type(.) != "MULTIPOINT") %>%
  #[2] were exclusively marine, 
  filter(MARINE != "2") %>% #0: 100% terrestrial, 1: both, 2: 100% marine
  #[3] were established after 2000 since the forest loss data range from 2001 to 2018,
  filter(STATUS_YR > 2000) %>%
  #[4] had area less than 1 km2 since forest change in small PAs can be hard to
  # estimate accurately, 
  filter(REP_AREA - REP_M_AREA >= 1) %>%
  st_write(dsn = wdpa_file_name)

# 48066 PAs filtered. Check as this seems quite low compared to 286356 in the 
# WDPA global database.
```

## Prepare data

The data from IUCN now comes in 2 parts for the reptiles.

```{r fix_ui}
#| eval: !expr from_scratch_mode

library(tidyverse)
library(sf)

reptiles1 <- st_read("data_input/range_maps/REPTILES_PART1.shp") 
  
reptiles1 %>%
  select(-OBJECTID) %>%
  st_write(dsn = "data_input/range_maps/REPTILES_PART2.shp", append = TRUE)

file.rename(list.files(path = "data_input/range_maps", pattern ="REPTILES_PART2",
                       full.names = TRUE),
            str_replace(list.files(path = "data_input/range_maps",
                                   pattern="REPTILES_PART2", full.names = TRUE), 
                        pattern="REPTILES_PART2", "REPTILES"))
```

## Get pre-processed milestones

```{r}
#| eval: !expr iterative_development_mode

library(dplyr)
library(stringr)
library(aws.s3)
library(purrr)
library(stringr)

if (Sys.info()["sysname"] == "Windows") {
  source("secrets_aws.R")
}

# Listing files in bucket
my_files <- get_bucket_df(bucket = "fbedecarrats",
                          prefix = "Replication_wolf",
                          region = "")
# Selecting tifs at root
my_tifs <- my_files %>%
  filter(str_ends(Key, ".tif")) %>%
  pluck("Key")

# Generating the destination paths
my_tifs_dest <- my_tifs %>%
  str_replace("data_processed/elev.tif", "data_processed/rasters/elev.tif") %>%
  str_replace("Replication_wolf/data_processed", "data_processed") %>%
  str_replace("Replication_wolf", "data_input")

# transfer_tif <- tibble(my_tifs, my_tifs_dest)

# A function to iterate/vectorize copy
save_from_s3 <- function(x, y) {
  aws.s3::save_object(
    object = x,
    bucket = "fbedecarrats",
    file = y,
    overwrite = FALSE,
    region = "")
  }
# copy from S3 to local
map2(my_tifs, my_tifs_dest, save_from_s3)

# Select PA shp
my_PAs <- my_files %>%
  filter(str_detect(Key, "/PAs/WDPA")) %>%
  pluck("Key")
# Create paths in local machine
my_PAs_dest <- my_PAs %>%
  str_replace("Replication_wolf/", "data_input/")
# Retrieve the data
map2(my_PAs, my_PAs_dest, save_from_s3)

# # Select WWF shp
# my_biomes <- my_files %>%
#   filter(str_detect(Key, "/official/")) %>%
#   pluck("Key")
# # Create paths in local machine
# my_biomes_dest <- my_biomes %>%
#   str_replace("Replication_wolf/", "data_input/")
# # Retrieve the data
# map2(my_biomes, my_biomes_dest, save_from_s3)

# # Create a function to put data from local machine to S3
# put_to_s3 <- function(x, y) {
#   aws.s3::put_object(
#     file = x,
#     object = y,
#     bucket = "fbedecarrats",
#     region = "",
#     multipart = TRUE)
# }
# 
# # Apply it to the PAs
# wdpa_local_files <- list.files("data_input/PAs", full.names = TRUE)
# wdpa_s3_objects <- wdpa_local_files %>%
#   str_replace("data_input", "Replication_wolf")
# map2(wdpa_local_files, wdpa_s3_objects, put_to_s3)
# # Also to the source
# put_to_s3("data_input/WDPA_Jan2023_Public.gdb.zip",
#           "Replication_wolf/WDPA_Jan2023_Public.gdb.zip")
# # Single put
# aws.s3::put_object(
#   file = "data_input/aau3445-Data-S3.tif",
#   object = "Replication_wolf/aau3445-Data-S3.tif",
#   bucket = "fbedecarrats",
#   region = "",
#   multipart = TRUE)
```


