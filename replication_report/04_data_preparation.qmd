---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Data gathering and preparation {#sec-data_preparation}

The replication of this study requires a careful data management. The input data only represents a volume of 25.5 GB and it has to be transformed once or twice before the statistical analysis. First, a folder structure must be created before running the original study source code.

```{r wet_wd}
if (!stringr::str_detect(getwd(), "PA_matching")) {
  setwd("PA_matching")
}
if (!stringr::str_ends(getwd(), "replication_report")) {
  setwd("replication_report")
}
```


```{r create_dirs}
# Create arborescence needed for code from Wolf et al. to run
dir.create("data_input")
dir.create("data_input/PAs")
dir.create("data_input/range_maps")
dir.create("temp")
dir.create("data_processed")
dir.create("data_processed/rasters")
dir.create("output")
```

## Code updates to fix automatic downloads

Several data sources are automatically fetched when running the source code, but several links and APIs are broken and need updating:

- The country borders are fetched using the package `{rnaturalearth}` but the source API has evolved and the official version of the package is broken. A development version must be installed from Github (see @sec-rsetup) ;
- The URL for Curtis et al. [-@curtis2018] data from the Science portal is broken and needs to be replaced.
- The URL for the WWF biome areas also needs to be updated.

The following code performs these modifications.

```{r replace_function}
library(tidyverse) # A library bundle to ease data management

# Not a good practice, only for debugging
# setwd("replication_report")
# URL pointing to the zip download of the Wolf et al. repository
repo <- "https://codeload.github.com/wolfch2/PA_matching/zip/refs/heads/master"

# downloads locally
download.file(repo, destfile = "original.zip", method = "curl")
# unzips to a containing folder named PA_matching-master (github default)
unzip("original.zip")
# original folder will remain intact, we make a copy with required edits to run
dir.create("PA_matching_asis")
file.copy(list.files("PA_matching-master", full.names = TRUE), 
          "PA_matching_asis", overwrite = TRUE)
file.remove("original.zip")

# A function to replace some parts of the code ---------------------------------
replace_all <- function(pattern, replacement, file) {
  readLines(file) %>%
    str_replace_all(pattern, replacement) %>%
    writeLines(file)
}

# Change the source paths ------------------------------------------------------
join_script <- "PA_matching_asis/004 - join_rasters.R"
# Replace Science URL that has changed
old <- "https://science.sciencemag.org/highwire/filestream/715492/field_highwire_adjunct_files/2/aau3445-Data-S3.tif"
new <- "https://www.science.org/action/downloadSupplement?doi=10.1126%2Fscience.aau3445&file=aau3445-data-s3.tif"
replace_all(pattern = old,
            replacement = new,
            file = join_script)
# Replace Curtis et al filename: lowercased by Science portal admins
replace_all(pattern = "data_input/aau3445-Data-S3.tif",
            replacement = "data_input/aau3445-data-s3.tif",
            file = join_script)
# Replace url to download WWF data
old <- "https://c402277.ssl.cf1.rackcdn.com/publications/15/files/original/official_teow.zip\\?1349272619"
new <- "https://files.worldwildlife.org/wwfcmsprod/files/Publication/file/6kcchn7e3u_official_teow.zip"
replace_all(pattern = old,
            replacement = new,
            file = join_script)
replace_all(pattern = "data_input/official_teow.zip\\?1349272619",
            replacement = "data_input/6kcchn7e3u_official_teow.zip",
            file = join_script)

# Prevent the scripts to modify the working directory
list.files("PA_matching_asis", full.names = TRUE) %>%
  map(\(x) replace_all("setwd", "# setwd", x))

```

## Data that needs to be fetched manually before running code

Several sources needs to be collected manually and stored in the file tree to run the code. After manual download stored these data in an encrypted archives on a private S3 storage. We also store the file encryption in a Vault instance.

```{r}

library(tidyverse)
library(aws.s3)

# To use the S3 server to save and load intermediate results,# You must enter 
# your AWS credentials in the R console. For more indications see: 
# https://www.book.utilitr.org/01_r_insee/fiche_utiliser_rstudio_sspcloud#renouveler-des-droits-dacc%C3%A8s-p%C3%A9rim%C3%A9s

# A function to put data from local machine to S3
put_to_s3 <- function(from, to) {
  aws.s3::put_object(
    file = from,
    object = to,
    bucket = "fbedecarrats",
    region = "",
    multipart = TRUE)
}

# A function to iterate/vectorize copy
save_from_s3 <- function(from, to) {
  aws.s3::save_object(
    object = from,
    bucket = "fbedecarrats",
    file = to,
    overwrite = FALSE,
    region = "")
}

if (Sys.getenv("AWS_ACCESS_KEY_ID") != "") {
  # Listing files in bucket
  my_files <- get_bucket_df(bucket = "fbedecarrats",
                            prefix = "Replication_wolf",
                            region = "")
}
```

### Species data

The published article mentions that the sources are publicly available, but several sources are not in open data, that means that they are not directly downloadable. This refers to three sources :

- the species range maps from IUCN (include mamals, amphibians and reptiles) must be fetched [from the IUCN Spatial data download webpage](https://www.iucnredlist.org/resources/spatial-data-download). They must be unzipped and placed in the folder `data_input/range_maps` ;
- For the bird species range map from Birds of the world, a query must be submitted via email to BirdLife  with the elements described on [the dedicated webpage](http://datazone.birdlife.org/species/requestdis). The authorization takes about two weeks to be assessed. If/after authorization, the administrators send the user a link to download the data. 
- It is also required to generate a token from the UICN website and include it in the line 18 of the script `002 - dl_IUCN.R` for it to run successfully.


```{r}
# Fetch and unzip species range maps -------------------------------------------

# We stored the range maps in encrypted zip (sensitive data)

# If running on SSP Cloud, getting the ecryption key from vault
if (Sys.info()["effective_user"] == "onyxia") {
  system("vault kv get -format=table onyxia-kv/fbedecarrats/species_info", 
         intern = TRUE) -> my_secret 
  my_key_zip <- my_secret %>%
    pluck(18) %>%
    str_extract("[:alnum:]*$")
  my_key_api <- my_secret %>%
    pluck(17) %>%
    str_extract("[:alnum:]*$")
} else { # otherwise you need to store your file encryption key 
         # in a file named secret_zip_key
 my_secret <- readLines("secret_zip_key")
}

# Load the data from S3 and decrypt it
save_from_s3(from = "Replication_wolf/data_input/range_maps/iucn_species_classes.zip",
             to = "iucn_species_classes.zip")
system(paste("unzip -P", my_key_zip, "iucn_species_classes.zip"))
file.remove("iucn_species_classes.zip")
save_from_s3(from = "Replication_wolf/data_input/range_maps/BOTW.zip",
             to = "BOTW.zip")
system(paste("unzip -P", my_key_zip, "BOTW.zip", " -d data_input/range_maps/"))
file.remove("BOTW.zip")

# Merge reptile files: uncomment to run ----------------------------------------

# # Note that the Reptile species range map comes in 2 parts that need to be
# # merged. The following code performs this operation
# 
# reptiles1 <- st_read("data_input/range_maps/REPTILES_PART1.shp")
# 
# reptiles1 %>%
#   select(-OBJECTID) %>%
#   st_write(dsn = "data_input/range_maps/REPTILES_PART2.shp", append = TRUE)
# 
# file.rename(list.files(path = "data_input/range_maps", 
#                        pattern ="REPTILES_PART2", full.names = TRUE),
#             str_replace(list.files(path = "data_input/range_maps",
#                                    pattern="REPTILES_PART2", 
#                                    full.names = TRUE),
#                         pattern="REPTILES_PART2", "REPTILES"))
```

### Data on protected areas

The Protected planet website only provides the latest version of the World database on protected areas (WDPA). The data can be fetched in shapefile, which is the format used for the original study. The original study used a WDPA version rom January 2020 in shapefile format f. This is not documented, but ended up finding the WDPA version from January 2020 in gdb format on a open AWS server used by UNEP-WCM. The data is difficult to access in GDB format, so we extract it and save it in geoparquet.

```{r}
#| eval: false
library(wdpar)
library(tidyverse)
library(sf)
library(geoarrow)

if (!stringr::str_ends(getwd(), "replication_report")) {
  setwd("replication_report")
}

# Found the previous version of WDPA
dest_wdpa_Jan2020 <- "data_input/PAs/WDPA_Jan2020_Public.zip"

if (!file.exists("data_input/PAs/WDPA_Jan2020_polygons.parquet")) {
  wdpa_Jan2020_url <- "https://pp-import-production.s3-eu-west-1.amazonaws.com/WDPA_Jan2020_Public.zip"
  options(timeout=3000) # Need to increase download timeout to get the data
  download.file(wdpa_Jan2020_url, destfile = dest_wdpa_Jan2020)
  
  unzip(dest_wdpa_Jan2020, exdir = "data_input/PAs")
  wdpa2020_poly <- read_sf(
    dsn = "data_input/PAs/WDPA_Jan2020_Public/WDPA_Jan2020_Public.gdb",
    layer = "WDPA_poly_Jan2020")
  write_geoparquet(wdpa2020_poly, "data_input/PAs/WDPA_Jan2020_polygons.parquet")
}
# save_from_s3("Replication_wolf/data_input/PAs/WDPA_Jan2020_polygons.parquet",
#              "data_input/PAs/WDPA_Jan2020_polygons.parquet")
```

## Fetch source data


### GDP

In the source code of the original study, the world bank data is fetched using a R package that interfaces with the API of the World Wank "World Development Indicators" (WDI) database, using the following code :

```{r}
#! eval: false
GDP_raw = wb(indicator="NY.GDP.PCAP.KD", startdate=2000, enddate=2018) # GDP per capita, PPP (current international $)
```
There a two main problems with this method:  

- The data for the corresponding year returned by the API is different. When the original study was submitted to Nature Ecology & Evolution (17 November 2019 according to its metadata), the GDP per capita indicator returned by the WDI API was based on the 2010 PPP. Since 2021, the GDP per capita indicator returned by the WDI API is based on the 2015 PPP. The data is not reproducible and using this method produces different data.
- The data for Venezuela has been removed from the WDI public database "due to the unavailability of data".

To avoid this issue, we use the Internet Archive to download a snapthot of the data from the 8 November 2019 avalable at [this persistent address](https://web.archive.org/web/20200916094145if_/http://api.worldbank.org/v2/en/indicator/NY.GDP.PCAP.KD?downloadformat=csv). The following prepares the data at the same format than the one outputted by the wbstat package to ensure it is identical with what was used for the initial study.

```{r}
download.file("https://web.archive.org/web/20200916094145if_/http://api.worldbank.org/v2/en/indicator/NY.GDP.PCAP.KD?downloadformat=csv", destfile = "temp/gdp_cap.zip", method = "curl")
unzip(zipfile = "temp/gdp_cap.zip", exdir = "temp")
GDP_raw <- read_csv("temp/API_NY.GDP.PCAP.KD_DS2_en_csv_v2_1345070.csv", 
                    skip = 3)

GDP_raw <- gdp_cap %>%
  dplyr::select(starts_with(c("Country", "Indicator", "2"))) %>%
  dplyr::select(-`2019`) %>%
  pivot_longer(cols = -starts_with(c("Country", "Indicator")),
               names_to = "date")

write_csv(GDP_raw, "GDP_PC_PPP2010_2000-2018.csv")

```


## Get pre-processed milestones

```{r}
# Selecting tifs at root
my_tifs <- my_files %>%
  filter(str_ends(Key, ".tif")) %>%
  pluck("Key")

# Generating the destination paths
my_tifs_dest <- my_tifs %>%
  str_replace("data_processed/elev.tif", "data_processed/rasters/elev.tif") %>%
  str_replace("Replication_wolf/data_processed", "data_processed") %>%
  str_replace("Replication_wolf", "data_input")

# copy from S3 to local
map2(my_tifs, my_tifs_dest, save_from_s3)


# # Select WWF shp
# my_biomes <- my_files %>%
#   filter(str_detect(Key, "/official/")) %>%
#   pluck("Key")
# # Create paths in local machine
# my_biomes_dest <- my_biomes %>%
#   str_replace("Replication_wolf/", "data_input/")
# # Retrieve the data
# map2(my_biomes, my_biomes_dest, save_from_s3)

# # Create a function to put data from local machine to S3
# put_to_s3 <- function(x, y) {
#   aws.s3::put_object(
#     file = x,
#     object = y,
#     bucket = "fbedecarrats",
#     region = "",
#     multipart = TRUE)
# }
# 
# # Apply it to the PAs
# wdpa_local_files <- list.files("data_input/PAs", full.names = TRUE)
# wdpa_s3_objects <- wdpa_local_files %>%
#   str_replace("data_input", "Replication_wolf")
# map2(wdpa_local_files, wdpa_s3_objects, put_to_s3)
# # Also to the source
# put_to_s3("data_input/WDPA_Jan2023_Public.gdb.zip",
#           "Replication_wolf/WDPA_Jan2023_Public.gdb.zip")
# # Single put
# aws.s3::put_object(
#   file = "data_input/aau3445-Data-S3.tif",
#   object = "Replication_wolf/aau3445-Data-S3.tif",
#   bucket = "fbedecarrats",
#   region = "",
#   multipart = TRUE)
```


