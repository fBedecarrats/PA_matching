---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Data gathering and preparation

The replication of this study requires a careful data management. The input data only represents a volume of 16.35 GB (without accounting for the data fetched through IUCN API) and it has to be transformed once or twice before the statistical analysis. First, a folder aborescence must be created before running Wolf et al. source code.

```{r create_dirs}
dir.create("data_input")
dir.create("data_input/PAs")
dir.create("data_input/range_maps")
dir.create("temp")
dir.create("data_processed")
dir.create("data_processed/rasters")
```

## Species data

The published article mentions that the sources are publicly available, but several sources are not open data, that means that they are not directly downloadable. This refers to three sources :

- the species range maps from IUCN (include mamals, amphibians and reptiles) must be fetched [from the IUCN Spatial data download webpage](https://www.iucnredlist.org/resources/spatial-data-download). They must be unzipped and placed in the folder `data_input/range_maps` ;
- For the bird species range map from Birds of the world, a query must be submitted via email to BirdLife  with the elements described on [the dedicated webpage](http://datazone.birdlife.org/species/requestdis). The authorization takes about two weeks to be assessed. If/after authorization, the administrators send the user a link to download the data. 
- It is also required to generate a token from the UICN website and include it in the line 18 of the script `002 - dl_IUCN.R` for it to run successfully.

In our case, we stored the species data in an encrypted archives on a private S3 storage. We also store the file encryption in a Vault instance.



## Data on protected areas

Protected planet only provides the latest version of the WDPA. We could not figure out how to obtain the versions from previous months or years. Wolf et al. used the January 2020 version, we use the January 2023. The dataset includes the year of obtention of the status, which enables to apply the same temporal filters than the authors (Protected areas created between from 2001 and 2019), but it is possible that information about existing entities have been added, modified or deleted between January 2020 and January 2021.


### Add required subfolders






## Fetch source data

We downloaded the data from protectedplanet.net and placed it in the folder `replication_report/temp`. We then executed the following script to have it in the right format.

get the right WDPA data. Wolf et al. use the data from January 2020.

```{r}
library(aws.s3)
save_from_s3(from = "Replication_wolf/WDPA_Jan2023_Public.gdb.zip",
             to = "data_input/WDPA_Jan2023_Public.gdb.zip")
save_from_s3(from = "Replication_wolf/data_input/PAs/WDPA_Jan2023_poly.parquet",
             to = "data_input/PAs/WDPA_Jan2023_poly.parquet")
```
```{r fetch_wdpa}
#| eval: !expr from_scratch_mode

library(tidyverse)
library(wdpar)
library(sf)

# Get the data from protectedplanet
if (file.exists("data_input/WDPA_Jan2023_Public.gdb.zip")) {
  wdpa_global <- wdpa_read("data_input/WDPA_Jan2023_Public.gdb.zip")
} else {
  wdpa_global <- wdpa_fetch("global", download_dir = "data_input")
}

# Build a name with same structure than Wolf et al.
wdpa_file_name <- list.files("data_input", pattern = ".gdb.zip") %>%
  str_remove("_Public.gdb.zip") %>%
  paste0("data_input/PAs/", ., "-shapefile-polygons.shp")
# Filter for polygons (filtering out points to be sure not to exclude something
# by mistake) and write locally.
wdpa_global %>%
  #"We excluded PAs from our primary analysis that: 
  #[1] had point (centroid) information only, 
  filter(st_geometry_type(.) != "MULTIPOINT") %>%
  #[2] were exclusively marine, 
  filter(MARINE != "2") %>% #0: 100% terrestrial, 1: both, 2: 100% marine
  #[3] were established after 2000 since the forest loss data range from 2001 to 2018,
  filter(STATUS_YR > 2000) %>%
  #[4] had area less than 1 km2 since forest change in small PAs can be hard to
  # estimate accurately, 
  filter(REP_AREA - REP_M_AREA >= 1) %>%
  st_write(dsn = wdpa_file_name)

# 48066 PAs filtered. Check as this seems quite low compared to 286356 in the 
# WDPA global database.
```

## Prepare data

The data from IUCN now comes in 2 parts for the reptiles.

```{r fix_ui}
#| eval: !expr from_scratch_mode

library(tidyverse)
library(sf)

reptiles1 <- st_read("data_input/range_maps/REPTILES_PART1.shp") 
  
reptiles1 %>%
  select(-OBJECTID) %>%
  st_write(dsn = "data_input/range_maps/REPTILES_PART2.shp", append = TRUE)

file.rename(list.files(path = "data_input/range_maps", pattern ="REPTILES_PART2",
                       full.names = TRUE),
            str_replace(list.files(path = "data_input/range_maps",
                                   pattern="REPTILES_PART2", full.names = TRUE), 
                        pattern="REPTILES_PART2", "REPTILES"))
```

## Get pre-processed milestones

```{r}
#| eval: !expr iterative_development_mode

library(dplyr)
library(stringr)
library(aws.s3)
library(purrr)
library(stringr)

if (Sys.info()["sysname"] == "Windows") {
  source("secrets_aws.R")
}

# Listing files in bucket
my_files <- get_bucket_df(bucket = "fbedecarrats",
                          prefix = "Replication_wolf",
                          region = "")
# Selecting tifs at root
my_tifs <- my_files %>%
  filter(str_ends(Key, ".tif")) %>%
  pluck("Key")

# Generating the destination paths
my_tifs_dest <- my_tifs %>%
  str_replace("data_processed/elev.tif", "data_processed/rasters/elev.tif") %>%
  str_replace("Replication_wolf/data_processed", "data_processed") %>%
  str_replace("Replication_wolf", "data_input")

# transfer_tif <- tibble(my_tifs, my_tifs_dest)

# A function to iterate/vectorize copy
save_from_s3 <- function(x, y) {
  aws.s3::save_object(
    object = x,
    bucket = "fbedecarrats",
    file = y,
    overwrite = FALSE,
    region = "")
  }
# copy from S3 to local
map2(my_tifs, my_tifs_dest, save_from_s3)

# Select PA shp
my_PAs <- my_files %>%
  filter(str_detect(Key, "/PAs/WDPA")) %>%
  pluck("Key")
# Create paths in local machine
my_PAs_dest <- my_PAs %>%
  str_replace("Replication_wolf/", "data_input/")
# Retrieve the data
map2(my_PAs, my_PAs_dest, save_from_s3)

# # Select WWF shp
# my_biomes <- my_files %>%
#   filter(str_detect(Key, "/official/")) %>%
#   pluck("Key")
# # Create paths in local machine
# my_biomes_dest <- my_biomes %>%
#   str_replace("Replication_wolf/", "data_input/")
# # Retrieve the data
# map2(my_biomes, my_biomes_dest, save_from_s3)

# # Create a function to put data from local machine to S3
# put_to_s3 <- function(x, y) {
#   aws.s3::put_object(
#     file = x,
#     object = y,
#     bucket = "fbedecarrats",
#     region = "",
#     multipart = TRUE)
# }
# 
# # Apply it to the PAs
# wdpa_local_files <- list.files("data_input/PAs", full.names = TRUE)
# wdpa_s3_objects <- wdpa_local_files %>%
#   str_replace("data_input", "Replication_wolf")
# map2(wdpa_local_files, wdpa_s3_objects, put_to_s3)
# # Also to the source
# put_to_s3("data_input/WDPA_Jan2023_Public.gdb.zip",
#           "Replication_wolf/WDPA_Jan2023_Public.gdb.zip")
# # Single put
# aws.s3::put_object(
#   file = "data_input/aau3445-Data-S3.tif",
#   object = "Replication_wolf/aau3445-Data-S3.tif",
#   bucket = "fbedecarrats",
#   region = "",
#   multipart = TRUE)
```


