---
title: "Reimplement matching"
editor: visual
editor_options: 
  chunk_output_type: console
---

## Trying a different computation strategy

We have not yet been able to successfully replicate the original study  as their code does not run successfully and generates errors. We identified the origin of some errors and corrected them, but we remained stuck with Julia errors. We tried different computing environment : local, cloud servers and with several different configurations (Windows, Linux, different R, python and Julia versions, etc.), and with large amounts of CPUs and memory.

We believe that the computing sequence elaborated by this study authors is particularly complex because of the very large amount of data to be processed. We propose a different approach that seems more straight forward and use the same entry parameters as Wolf et al. (2021).

Here we attempt to reproduce every analysis steps and parameters of Wolf et al. (2021) but to implement a different data processing strategy that:

-   implies less manipulations steps to be less error prone;
-   only relies on one language and configuration for local processing.
-   relies on cloud data processing for very big datasets: we leverage the cloud computing platform used by the authors to download their source data (Google earth engine) to do more than only data acquisition: we use it also to preprocess and combine the source data before download;;
-   takes advantage that the matching algorithm segments by country and biome to process the data in smaller batches that are easier to process on commonly accessible processing configurations.
-   containerize and archive our computing environement to enable any future researcher to reproduce it in the future.

```{r}
# We need the latest version of rnaturalearth
if (!"rnaturalearth" %in% installed.packages() | 
    packageVersion("rnaturalearth") <= "0.3.2") {
  remotes::install_github("https://github.com/ropensci/rnaturalearth")
}
library(tidyverse)
library(sf)
library(tmap)
library(geodata)
library(rnaturalearth)
library(units)
library(terra)
library(stars)
if (str_ends(getwd(), "replication_report", negate = TRUE)) {
  setwd("replication_report")
}
```

## Fetch and prepare data from other sources than GEE

We gather data for national boundaries and biomes and combine it.

```{r}
#| fig-cap: "Areas of interest (biome segmented along national boundaries)"

# Countries  using geodata (ie. GADM), rnaturalearth is broken
dir.create("revamp")
dir.create("revamp/countries")

# This data is from GADM which seems more complete and accurate
countries <- world(resolution=2, level=0, path = "revamp/countries")
countries <- countries %>%
  st_as_sf() %>%
  st_make_valid() %>%
  mutate(country_num = 1:nrow(.), .before = GID_0)
# Fetch data from rnaturalearth for continents
countries2 <- ne_download(scale = 10,
                         type = "countries",
                         category = "cultural",
                         destdir = "revamp/countries",
                         load = TRUE,
                         returnclass = "sf")
country_continent <- countries2 %>%
  st_drop_geometry() %>%
  select(ADM0_A3, continent = CONTINENT)
countries <- countries %>%
  left_join(country_continent, join_by(GID_0 == ADM0_A3)) %>%
  mutate(continent = case_when(
    NAME_0 == "Åland" ~ "Europe", # some territories are missing
    NAME_0 == "Bonaire, Saint Eustatius and Saba" ~ "North America",
    NAME_0 == "Caspian Sea" ~ "Europe",
    NAME_0 == "Christmas Island" ~ "Oceania",
    NAME_0 == "French Guiana" ~ "South America",
    NAME_0 == "Guadeloupe" ~ "North America",
    NAME_0 == "Kosovo" ~ "Europe",
    NAME_0 == "Martinique" ~ "North America",
    NAME_0 == "Mayotte" ~ "Africa",
    NAME_0 == "Paracel Islands" ~ "Asia",
    NAME_0 == "Réunion" ~ "Africa",
    NAME_0 == "South Sudan" ~ "Africa",
    .default = continent),
    continent = ifelse(continent == "Seven seas (open ocean)",
                       "Open ocean", continent))
# We prepare biome polygons (only at first run, if not already done)
if (!file.exists("revamp/aois/aois.gpkg")) {
  # Fetch biomes
  dir.create("revamp/biomes")
  if (!file.exists("revamp/biomes/wwf_biomes.zip")) {
    wwf_url <- paste0("https://files.worldwildlife.org/wwfcmsprod/files/", 
                      "Publication/file/6kcchn7e3u_official_teow.zip")
    download.file(wwf_url, "revamp/biomes/wwf_biomes.zip")
    unzip("revamp/biomes/wwf_biomes.zip", exdir = "revamp/biomes")
  }
  # Merge ecoregions with the same biome
  biomes <- st_read("revamp/biomes/official/wwf_terr_ecos.shp") %>%
    st_make_valid() %>%
    group_by(BIOME) %>%
    summarise()
# Rename according to documentation and filter
  biomes <- biomes %>%
    rename(biome_num = BIOME) %>%
    mutate(biome_name = case_when(
      biome_num == 1 ~ "Tropical & Subtropical Moist Broadleaf Forests",
      biome_num == 2 ~ "Tropical & Subtropical Dry Broadleaf Forests",
      biome_num == 3 ~ "Tropical & Subtropical Coniferous Forests",
      biome_num == 4 ~ "Temperate Broadleaf & Mixed Forests",
      biome_num == 5 ~ "Temperate Conifer Forests",
      biome_num == 6 ~ "Boreal Forests/Taiga",
      biome_num == 7 ~ "Tropical & Subtropical Grasslands, Savannas & Shrublands",
      biome_num == 8 ~ "Temperate Grasslands, Savannas & Shrublands",
      biome_num == 9 ~ "Flooded Grasslands & Savannas",
      biome_num == 10 ~ "Montane Grasslands & Shrublands",
      biome_num == 11 ~ "Tundra",
      biome_num == 12 ~ "Mediterranean Forests, Woodlands & Scrub",
      biome_num == 13 ~ "Deserts & Xeric Shrublands",
      biome_num == 14 ~ "Mangroves",
      .default = "Unknown"), .before = geometry)
  my_biomes <- biomes %>%
    filter(biome_name %in% c("Tropical & Subtropical Moist Broadleaf Forests",
                             "Tropical & Subtropical Dry Broadleaf Forests",
                             "Tropical & Subtropical Coniferous Forests",
                             "Temperate Broadleaf & Mixed Forests",
                             "Temperate Conifer Forests",
                             "Boreal Forests/Taiga",
                             "Mangroves")) %>%
    mutate(biome_num = case_when(
      biome_name == "Tropical & Subtropical Moist Broadleaf Forests" ~ 1,
      biome_name == "Tropical & Subtropical Dry Broadleaf Forests" ~ 2,
      biome_name == "Tropical & Subtropical Coniferous Forests" ~ 3,
      biome_name == "Temperate Broadleaf & Mixed Forests" ~ 4,
      biome_name == "Temperate Conifer Forests" ~ 5,
      biome_name == "Boreal Forests/Taiga" ~ 6,
      biome_name == "Mangroves" ~ 7))
  
  # Areas of interest (AOIs) combine countries and biomes
  aois <- st_intersection(countries, my_biomes) %>%
    mutate(area = st_area(.)) %>%
    arrange(area) # to start with the smallest for testing
  # save as geoparquet
  st_write(aois, "revamp/aois/aois.gpkg") 
} else {
  aois <- st_read("revamp/aois/aois.gpkg")
}

aois %>%
  rename(Biome = biome_name) %>%
  tm_shape() +
  tm_polygons(col = "Biome") + 
  tm_borders() + 
  tm_layout(legend.outside = TRUE,
            legend.outside.position = "right")
```

We obtain `r nrow(aois)` polygons of country/biomes combinations.

We also download the data from [@curtis2018] on deforestation drivers, which is a raster data.

```{r}
# We only execute this once
if (!file.exists("revamp/drivers/curtis_et_al_orig.tif")) {
  science_url <- paste0("https://www.science.org/action/downloadSupplement?",
                        "doi=10.1126%2Fscience.aau3445&file=aau3445-data-s3.tif")
  download.file(url = science_url, 
                destfile = "revamp/drivers/curtis_et_al_orig.tif",
                method = "curl")
  # load country + biome data
drivers <- rast("revamp/drivers/curtis_et_al_orig.tif") %>%
  project("epsg:4326") %>%
  writeRaster("revamp/drivers/drivers_curtis.tif")
}
```

## Move external data to GEE and prepare all information there

The first step of the data analysis workflow of Wolf et al. consisted in fetching data from Google Earth Engine. They downloaded each dataset (forest cover, forest loss, forest gain, elevation, slope, travel time and population density) as separate raster files to process them locally on their own computer. Because of the massive amount of such data, the subsequent steps in their analysis workflow are complex, rely from different languages, a large number of libraries (several of them now deprectated), fails to run and we are unable to debug it.

The Google Earth Engine can be leveraged to do much more than only downloading data, and therefore we plan to re-implement on this cloud environment the data preparation steps, instead of doing it locally on individual machines.

We use the R package `rgee` as an interface to Google Earth engine API [@aybar2020].

```{r}
# Sometime rgee does not find the right python env, so we specify it
reticulate::use_python("C:/Users/fbede/AppData/Local/r-miniconda/envs/rgee")
library(rgee) # accesses GEE through its python API
library(rgeeExtra)
ee_Initialize(user = "ndef", drive = TRUE, )
dir.create("revamp/gee")
```

We start by uploading the country, biome and deforestation drivers' data prepared in the previous step.

```{r}
#| eval: false

# We add this manually, as we had issues with API auth. protocol for GSC
drivers_curtis <- ee$Image("projects/ee-fbedecarrats/assets/drivers_curtis")
# We also load the AOIs manually due to the same problem
if (!file.exists("revamp/aois/aois_shp.zip")) {
  dir.create("revamp/aois")
  st_write(aois, "revamp/aois/aois.shp")
  aoi_files = list.files(path = "revamp/aois", pattern = "aois.*", 
                         full.names = TRUE)
  zip(zipfile = "revamp/aois/aois_shp.zip", files = aoi_files)
}
aois_ee <- ee$FeatureCollection("projects/ee-fbedecarrats/assets/aois")
# We get the colors used by R using tmaptools::palette_explorer()
biome_fills <- ee$Image()$byte()$
  paint(featureCollection = aois_ee,
        color = "biome_num")
biome_nation_borders <- ee$Image()$byte()$
  paint(featureCollection = aois_ee,
        color = 1,
        width = 1)

r_map_palette <- c("b3de69", # light green: Tropical & Subtropical Moist Broadleaf Forests 
                   "fdb462", # light orange: Tropical & Subtropical Dry Broadleaf Forests
                   "80b1d3", # blue: Tropical & Subtropical Coniferous Forest
                   "bebada", # purple: Temperate Broadleaf & Mixed Forests
                   "fb8072", # dark orange: Temperate Conifer Forests
                   "8dd3c7", # turquois: Boreal Forests/Taiga
                   "ffffb3") # yellow: Mangroves
Map$addLayer(biome_fills, list(min = 0, max = 6, palette = r_map_palette)) +
  Map$addLayer(biome_nation_borders, list(palette = "000000"))
```

![Diplay of biomes segmented by national boarders on google earth engine](img/biomes_national_rgee.png) \# Clip Hansen data with biome polygons

Instead of downloading and processing all the GFC data like Wolf and al., we instead keep only the data located within the perimeter of the biomes of interest.

```{r}
# Hansen/GFC -------------------------------------------------------------------
gfc <- ee$Image("UMD/hansen/global_forest_change_2018_v1_6")

# We save the output as image to display it in quarto
gfc_mask <- gfc$select("datamask")$eq(1)
gfc_wolf <-  gfc$updateMask(gfc_mask)
Map$addLayer(gfc_wolf, list(
  bands = c("loss", "treecover2000", "gain"),
  max = c(1, 255, 1)))
```

![GFC data exported by wolf to be processed on a local computer (green: tree cover in 2000, red: tree cover loss 2001-2018, blue: tree cover gain 2001-2012)](img/gfc_data_exported_by_wolf.png)
```{r}
# We save the output as image to display it in quarto
gfc_aois <- gfc_wolf$clipToCollection(aois_ee)
Map$addLayer(gfc_aois, list(
  bands = c("loss", "treecover2000", "gain"),
  max = c(1, 255, 1)))

```
![GFC data restricted to areas of sudy to export for matching (green: tree cover in 2000, red: tree cover loss 2001-2018, blue: tree cover gain 2001-2012)](img/gfc_data_on_aois.png)
# Stack information as bands in a single raster

Wolf et al. downloaded each information source in a separate raster, that they later on joined. We find easier and less error-prone to stack all the information as bands of the same raster. The data might be downloaded in separate files for different national segment of each biome, but for the same regions, all the complementary information is attached to the same pixels with the same resolution (alias, a "clean" table).

```{r}
# This function takes as an enty a GEE FeatureCollections and returns an GEE
# Image with 13 bands with forest cover, forest loss, year of forest loss, 
# elevation, slope, travel time to a city, population density, country number,
# biome number, a dummy if it is a PA, or in a 10km distance of a PA, and the 
# WPDA ID number of the PA if it is a PA. 
prepare_gee_data <- function(ee_region) {
  gfc <- ee$Image("UMD/hansen/global_forest_change_2018_v1_6")
  gfc_mask <- gfc$select("datamask")$eq(1)
  gfc_masked <-  gfc$updateMask(gfc_mask)
  
  # GFC data
  aoi_gfc <- gfc_masked$clipToCollection(ee_region)
  aoi_cover <- aoi_gfc$select("treecover2000")$
    reduceResolution(reducer = ee$Reducer$mean(), bestEffort = TRUE)$toInt()
  aoi_loss <- aoi_gfc$select("loss")$
    reduceResolution(reducer = ee$Reducer$mean(), bestEffort = TRUE)$toInt()
  aoi_lossyear <- aoi_gfc$select("lossyear")
  aoi_lossyear <- aoi_lossyear$updateMask(aoi_lossyear$neq(0))$
    reduceResolution(reducer = ee$Reducer$mode(maxRaw = 20), bestEffort = TRUE)$
    toInt()
  aoi_image <- aoi_cover$addBands(c(aoi_loss, aoi_lossyear))
  
  # elevation # 28min
  aoi_image <- aoi_image$addBands(ee$Image("USGS/GTOPO30")) # Elevation
  aoi_image[[4]] <- aoi_image[[4]]$toInt32()
  # slope
  aoi_image <- aoi_image$addBands(ee$Terrain$slope(aoi_image[[4]]))
  aoi_image[[5]] <- aoi_image[[5]]$toInt()
  # travel_time
  aoi_image <- aoi_image$addBands(
    ee$Image("Oxford/MAP/accessibility_to_cities_2015_v1_0"))
  aoi_image[[6]] <- aoi_image[[6]]$toInt()
  # Population density
  gpw <- paste0("CIESIN/GPWv411/GPW_UNWPP-Adjusted_Population_Density/",
                "gpw_v4_population_density_adjusted_to_2015_",
                "unwpp_country_totals_rev11_2000_30_sec")
  aoi_image <- aoi_image$addBands(ee$Image(gpw)$
                                    select("unwpp-adjusted_population_density"))
  aoi_image[[7]] <- aoi_image[[7]]$focal_mean(radius = 20e3,
                                              kernelType = "circle",
                                              units = "meters")$toInt()
  # We add countries and biomes
  reg_country_img <- aois_ee$filterBounds(ee_region$geometry())$
    reduceToImage(properties = list("cntry_n"), reducer = ee$Reducer$first())
  aoi_image <- aoi_image$addBands(reg_country_img$toInt())
  reg_biome_img <- aois_ee$filterBounds(ee_region$geometry())$
    reduceToImage(properties = list("biome_num"), reducer = ee$Reducer$first())
  aoi_image <- aoi_image$addBands(reg_biome_img$toInt())

  # We add information on protected areas
  wdpa_reg <- ee$FeatureCollection("WCMC/WDPA/current/polygons")$
    filterBounds(ee_region$geometry())$
    select("MARINE == 0 $$ STATUS != 'Proposed' && GIS_AREA >= 1")
  wdpa_reg <- wdpa_reg$map(function(pa) {
    return(pa$set("PA", 1))
  })
  
  pa_footprint <-wdpa_reg$reduceToImage(properties = list("PA"),
                           reducer = ee$Reducer$first())$toInt()
  pa_buffer <- pa_footprint$focal_max(10000, "circle", "meters")
  aoi_image <- aoi_image$addBands(c(pa_footprint, pa_buffer))
  aoi_image <- aoi_image$addBands(
    wdpa_reg$reduceToImage(properties = list("WDPAID"),
                           reducer = ee$Reducer$first())$toInt())
  aoi_image <- aoi_image$addBands(
    wdpa_reg$reduceToImage(properties = list("STATUS_YR"),
                           reducer = ee$Reducer$first())$toInt())
  names(aoi_image) <- c("cover", "loss", "lossyear", "elev", "slope",
                        "travel_time", "pop_dens", "country_num", "biome_num",
                        "is_pa", "is_pa_buffer", "wdpa_id", "status_year")
  # ADD CLIP /!\
  aoi_image <- aoi_image$clipToCollection(ee_region)
}

```

The previous code creates for each area of interest a raster with the following bands: cover, loss, lossyear, elev, slope, travel_time, pop_dens, PA, WDPA ID number, PA_status_yr, PA_buffer, country, biome.


Now we will iterate to download this raster in separated file, with one file per national segment of each biome of interest.

By continent
```{r}
# This loops over continents, but the data is only extracted for areas that
# are within a biome of study.
continents <- unique(aois$continent)
gee_tasks <- tibble(continent = continents)
# each continent takes 12 to 50 minutes to process. They do it in parallel.
for (i in 1:length(continents)) {
  print(continents[i])
  filename_continent <- str_replace(continents[i], " ", "_")
  aoi_region <- aois_ee$filter(paste0("contnnt == '", continents[i],"'"))
  aoi_extract <- prepare_gee_data(aoi_region)
  gee_tasks$task[[i]] <- ee_as_raster(
    aoi_extract, via = "drive", container = "rgee_batch", lazy = TRUE,
    dsn = paste0("revamp/rgee/", filename_continent, ".tif"),
    skipEmptyTiles = TRUE, scale = 1000,
    region = aoi_region$geometry())
}
# We fetch the data once it is ready
ee_utils_future_value(gee_tasks$task[[5]])
for (j in 1:length(continents)) {

}

```


```{r}
# This loops over countries, but the data is only extracted for areas that
# are within a biome of study.
my_countries <- unique(aois$GID_0)
country_tasks <- tibble(country = my_countries)
# each continent takes between less than 1 and XX minutes to process. 
# The computation runs in parallel.
for (i in 1:length(my_countries)) {
  GID <- my_countries[i]
  print(paste0(GID, " ", i, "/", length(my_countries)))
  aoi_country <- aois_ee$filter(paste0("GID_0 == '", GID,"'"))
  aoi_extract <- prepare_gee_data(aoi_country)
  country_tasks$task[[i]] <- ee_as_raster(
    aoi_extract, via = "drive", container = "rgee_country", lazy = TRUE,
    dsn = paste0("revamp/gee/", GID, ".tif"),
    skipEmptyTiles = TRUE, scale = 1000,
    region = aoi_country$geometry())
}

# We fetch the data once it is ready
for (j in 1:length(my_countries)) {
  ee_utils_future_value(country_tasks$task[[j]])
}
```

```{r}
aois <-aois %>%
  mutate(row_num = 1:nrow(aois), .before = everything())

aoi_num <- nrow(aois) # 364 areas of interest

# Loop over each AOI to generate a GEE task to extract and prepare the data
# over the corresponding geometry. The output are "future values" stored in
# one field. Invoking theses future values when GEE is done processing will
# fetch the corresponding rasters and store them locally
for (i in 26:aoi_num) {
  aoi <- slice(aois, i) %>%
    st_drop_geometry()
  print(paste("Process", aoi["biome_name"], "for", aoi["NAME_0"], 
              paste(i, aoi_num, sep = "/")))
  # Select the region to reduce over
  
  aoi_region <- aois_ee$filter(paste0("GID_0 == '", aoi["GID_0"],
                                      "' && biome_num == ", aoi["biome_num"]))
  aoi_extract <- prepare_gee_data(aoi_region)
  
  aois$gee_task[[i]] <- ee_as_raster(
    aoi_extract, via = "drive", container = "rgee_batch", lazy = TRUE,
    dsn = paste0("revamp/rgee/", aoi["GID_0"], "-", aoi["biome_num"], ".tif"),
    skipEmptyTiles = TRUE, scale = 1000,
    region = aoi_region$geometry())
}
# write_rds(aois, "aois_milestone.rds")
dir.create("revamp/rgee")
j <- 3

for (j in 26:aoi_num) {
  get_aoi <- slice(aois, j) %>%
    st_drop_geometry()
 my_raster <- ee_utils_future_value(unlist(aois$gee_task[[j]]))
 my_df <- as.data.frame(my_raster) %>%
   filter(country)
}


```




```{r}
aois_tbl <- aois %>%
  st_drop_geometry() %>%
  mutate(row_num = 1:nrow(aois), .before = everything())

aoi_num <- nrow(aois_tbl)
i <- 162
aoi <- slice(aois_tbl, i)
print(paste("Process", aoi["biome_name"], "for", aoi["NAME_0"], 
            paste(i, aoi_num, sep = "/")))
# Select the region to reduce over
ee_region <- aois_ee$filter(paste0("GID_0 == '", aoi["GID_0"],
                                   "' && biome_num == ", aoi["biome_num"]))



for (i in 1:3) {
  aoi <- slice(aois_tbl, i)
  print(paste("Process", aoi["biome_name"], "for", aoi["NAME_0"], 
              paste(i, aoi_num, sep = "/")))
  # Select the region to reduce over
  ee_region <- aois_ee$filter(paste0("GID_0 == '", aoi["GID_0"],
                                     "' && biome_num == ", aoi["biome_num"]))
  aois_tbl$task[i] <- ee_as_raster(
    gfc_aois, via = "drive", container = "rgee_test1", lazy = TRUE,
    region = ee_region$geometry(), crs = "EPSG:4326", maxPixels = 1e13,
    dsn = paste0("revamp/rgee/", aoi["GID_0"], "-", aoi["biome_num"], 
                 ".tif"),
    crsTransform = paste(scale, 0, -180, 0, -scale, 90, sep = ", "),
    dimensions = paste(as.character(round(360/scale)), "x",
                       as.character(round(360/scale))))
}

```



```{r}
# Test
wdpa_mdg <- ee$FeatureCollection("WCMC/WDPA/current/polygons")$
  filter("ISO3 == 'MDG'")
wdpa_mdg_buffer <- wdpa_mdg$geometry()$buffer(10000)
wdpa_mdg_buffer <- ee$Feature(wdpa_mdg_buffer)$
  set("PA_buffer", 1)
ee_print(wdpa_mdg_buffer)

```


# Add countries and PAs

```{r}



```

# Load deforestation drivers

```{r}

# Drivers

wwf_ecoregions %>%
  st_drop_geometry() %>%
  group_by(BIOME) %>%
  summarise(n_distinct = n_distinct(BIOME),
            n = n())
wwf_ecoregions %>%
  st_drop_geometry() %>%
  group_by(ECO_NAME) %>%
  summarise(n_distinct = n_distinct(ECO_NAME),
            n = n())

```

Load GEE data

```{r}


```
