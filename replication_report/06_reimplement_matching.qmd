---
title: "Reimplement matching"
editor: visual
editor_options: 
  chunk_output_type: console
---

## Trying a different computation strategy

We have not yet been able to successfully replicate Wolf et al. (2021) as their code does not run successfully and generates errors. We identified the origin of some errors and corrected them, but we remained stuck with Julia errors. We tried different computing environment : local, cloud servers and with several different configurations (Windows, Linux, different R, python and Julia versions, etc.), and with large amounts of CPUs and memory.

We believe that the computing sequence elaborated by this study authors is particularly complex because of the very large amount of data to be processed. We propose a different approach that seems more straight forward and use the same entry parameters as Wolf et al. (2021).

Here we attempt to reproduce every analysis steps and parameters of Wolf et al. (2021) but to implement a different data processing strategy that:

-   implies less manipulations steps to be less error prone;
-   only relies on one language and configuration for local processing.
-   relies on cloud data processing for very big datasets: we leverage the cloud computing platform used by the authors to download their source data (Google earth engine) to do more than only data acquisition: we use it also to preprocess and combine the source data before download;;
-   takes advantage that the matching algorithm segments by country and biome to process the data in smaller batches that are easier to process on commonly accessible processing configurations.
-   containerize and archive our computing environement to enable any future researcher to reproduce it in the future.

```{r}
setwd("replication_report")
library(tidyverse)
library(sf)
library(tmap)
library(geoarrow)
library(geodata)
library(units)
library(terra)
library(stars)
```

## Fetch and prepare data from other sources than GEE

We gather data for national boundaries and biomes and combine it.

```{r}
# Countries  using geodata (ie. GADM), rnaturalearth is broken
dir.create("revamp")
dir.create("revamp/countries")
countries <- world(resolution=2, level=0, path = "revamp/countries") %>%
  st_as_sf() %>%
  st_make_valid() %>%
  mutate(country_num = 1:nrow(.), .before = GID_0)

# Fetch and consolidate biomes
dir.create("revamp/biomes")
if (!file.exists("revamp/biomes/wwf_biomes.zip")) {
  wwf_url <- paste0("https://files.worldwildlife.org/wwfcmsprod/files/", 
                    "Publication/file/6kcchn7e3u_official_teow.zip")
  download.file(wwf_url, "revamp/biomes/wwf_biomes.zip")
  unzip("revamp/biomes/wwf_biomes.zip", exdir = "revamp/biomes")
}

biomes <- st_read("revamp/biomes/official/wwf_terr_ecos.shp") %>%
  st_make_valid() %>%
  group_by(BIOME) %>%
  summarise()

# Rename according to documentation and filter
biomes <- biomes %>%
  rename(biome_num = BIOME) %>%
  mutate(biome_name = case_when(
    biome_num == 1 ~ "Tropical & Subtropical Moist Broadleaf Forests",
    biome_num == 2 ~ "Tropical & Subtropical Dry Broadleaf Forests",
    biome_num == 3 ~ "Tropical & Subtropical Coniferous Forests",
    biome_num == 4 ~ "Temperate Broadleaf & Mixed Forests",
    biome_num == 5 ~ "Temperate Conifer Forests",
    biome_num == 6 ~ "Boreal Forests/Taiga",
    biome_num == 7 ~ "Tropical & Subtropical Grasslands, Savannas & Shrublands",
    biome_num == 8 ~ "Temperate Grasslands, Savannas & Shrublands",
    biome_num == 9 ~ "Flooded Grasslands & Savannas",
    biome_num == 10 ~ "Montane Grasslands & Shrublands",
    biome_num == 11 ~ "Tundra",
    biome_num == 12 ~ "Mediterranean Forests, Woodlands & Scrub",
    biome_num == 13 ~ "Deserts & Xeric Shrublands",
    biome_num == 14 ~ "Mangroves",
    .default = "Unknown"), .before = geometry)
my_biomes <- biomes %>%
  filter(biome_num %in% c(1, 2, 3, 4, 5, 6, 14))

# Areas of interest (AOIs) combine countries and biomes
aois <- st_intersection(countries, my_biomes) %>%
  mutate(area = st_area(.)) %>%
  arrange(area) # to start with the smallest for testing

tm_shape(aois) +
  tm_polygons(col = "biome_name") + 
  tm_borders() + 
  tm_legend()
```

We obtain `r nrow(aois)` polygons of country/biomes combinations.

We also download the data from [@curtis2018] on deforestation drivers, which is a raster data.

```{r}
if (!file.exists("revamp/drivers/curtis_et_al.tif")) {
  science_url <- paste0("https://www.science.org/action/downloadSupplement?",
                        "doi=10.1126%2Fscience.aau3445&file=aau3445-data-s3.tif")
  download.file(url = science_url, destfile = "revamp/drivers/curtis_et_al.tif",
                method = "curl")
}
```


## Move external data to GEE and prepare all information there

The first step of the data analysis workflow of Wolf et al. consisted in fetching data from Google Earth Engine. They downloaded each dataset (forest cover, forest loss, forest gain, elevation, slope, travel time and population density) as separate raster files to process them locally on their own computer. Because of the massive amount of such data, the subsequent steps in their analysis workflow are complex, rely from different languages, a large number of libraries (several of them now deprectated), fails to run and we are unable to debug it.

The Google Earth Engine can be leveraged to do much more than only downloading data, and therefore we plan to re-implement on this cloud environment the data preparation steps, instead of doing it locally on individual machines.

We use the R package `rgee` as an interface to Google Earth engine API [@aybar2020].

```{r}
# Sometime rgee does not find the right python env, so we specify it
reticulate::use_python("C:/Users/fbede/AppData/Local/r-miniconda/envs/rgee")
library(rgee) # accesses GEE through its python API
library(tidyverse)
ee_Initialize(gsc = TRUE)
dir.create("gee")
```

We start by uploading the country, biome and deforestation drivers' data prepared in the previous step. 

```{r}
# load country + biome data
drivers <- rast("revamp/drivers/curtis_et_al.tif") %>%
  project("epsg:4326") %>%
  writeRaster("revamp/drivers/drivers_curtis.tif")
# We add this manually, as we had issues with API auth. protocol for GSC
drivers_curtis <- ee$Image("projects/ee-fbedecarrats/assets/drivers_curtis")
# We also load the AOIs manually due to the same problem
dir.create("revamp/aois")
st_write(aois, "revamp/aois/aois.shp")
aoi_files = list.files(path = "revamp/aois", pattern = "aois.*", 
                       full.names = TRUE)
zip(zipfile = "revamp/aois/aois_shp.zip", files = aoi_files)


tif <- system.file("tif/L7_ETMs.tif", package = "stars")
x <- read_stars(tif)
assetId <- sprintf("%s/%s",ee_get_assethome(),'stars_l7')
ee_raster_02 <- raster_as_ee(
  x = x,
  assetId = assetId,
  overwrite = TRUE,
  bucket = "rgee_dev"
)


```



# Load Biomes on Google Earth Engine and use them as masks

First we will fetch the biome descriptions from the resolve ecoregions.


```{r}
# Get ecoregions that include WWF biomes
all_ecoregions <- ee$FeatureCollection("RESOLVE/ECOREGIONS/2017")
# list biome names
all_biome_names <- ee$List(all_ecoregions$aggregate_array("BIOME_NAME"))$
  distinct() # keep only 1 per modality
biome_names$getInfo() # print
```

     [1] "Tropical & Subtropical Moist Broadleaf Forests"          
     [2] "Tropical & Subtropical Grasslands, Savannas & Shrublands"
     [3] "Flooded Grasslands & Savannas"                           
     [4] "Montane Grasslands & Shrublands"                         
     [5] "Deserts & Xeric Shrublands"                              
     [6] "Mangroves"                                               
     [7] "Mediterranean Forests, Woodlands & Scrub"                
     [8] "Tropical & Subtropical Dry Broadleaf Forests"            
     [9] "Tropical & Subtropical Coniferous Forests"               
    [10] "Temperate Broadleaf & Mixed Forests"                     
    [11] "Temperate Conifer Forests"                               
    [12] "Temperate Grasslands, Savannas & Shrublands"             
    [13] "N/A"                                                     
    [14] "Boreal Forests/Taiga"                                    
    [15] "Tundra"  

Wolf et al. specify that they focused on the following ones:

-   Tropical & Subtropical Moist Broadleaf Forests
-   Tropical & Subtropical Dry Broadleaf Forests
-   Tropical & Subtropical Coniferous Forests
-   Temperate Broadleaf & Mixed Forests
-   Temperate Conifer Forests
-   Boreal Forests/Taiga
-   Mangroves

```{r}
# Filter selected biomes
select_ecoregs <- all_ecoregions$
  filter("BIOME_NAME == 'Tropical & Subtropical Moist Broadleaf Forests' ||
          BIOME_NAME == 'Tropical & Subtropical Dry Broadleaf Forests' ||
          BIOME_NAME == 'Tropical & Subtropical Coniferous Forests' ||
          BIOME_NAME == 'Temperate Broadleaf & Mixed Forests' ||
          BIOME_NAME == 'Temperate Conifer Forests' ||
          BIOME_NAME == 'Boreal Forests/Taiga' ||
          BIOME_NAME == 'Mangroves'")

```

![selected ecoregions from google earth engine](img/selected_ecoregions.png)

# Clip Hansen data with biome polygons

Instead of downloading and processing all the GFC data like Wolf and al., we instead keep only the data located within the perimeter of the biomes of interest.

```{r}
scale <- 0.008983153 # From Wolf et al. 2021
drive_folder <- "rgee"
local_folder <- "temp"

# Hansen/GFC -------------------------------------------------------------------
gfc <- ee$Image("UMD/hansen/global_forest_change_2018_v1_6")

# Cover other try
gfc_mask <- gfc$select("datamask")$eq(1)
cover <-  gfc$select("treecover2000")$
  updateMask(gfc_mask)$
  clipToCollection(select_ecoregs)
Map$addLayer(cover, list(bands = "treecover2000"), "green")
```

![forest data from selected ecoregions](img/filtered_forest_data.png)

# Stack information as bands in a single raster

Wolf et al. downloaded each information source in a separate raster, that they later on joined. We find easier and less error-prone to stack all the information as bands of the same raster. The data might be downloaded in separate files for different regions, but for the same regions, all the complementary information is attached to the same pixels with the same resolution (alias, a "clean" table).

```{r}
## Lossyear ------------------------------------
lossyear <- gfc$select("lossyear")
lossyear <- lossyear$updateMask(lossyear$neq(0))$ # maskout 0
  reduceResolution(reducer = ee$Reducer$mode(), 
                                      bestEffort = TRUE)$
  toInt()

## Loss --------------------------------------------
loss <- gfc$select("loss")$
  reduceResolution(reducer = ee$Reducer$mean(), bestEffort = TRUE)$
  toInt() # or to Int32?

# Other rasters fetched from GEE by wold et al. --------------------------------
elev <- ee$Image("USGS/GTOPO30")$
  toInt()

slope <- ee$Terrain$slope(elev)

travel_time <- ee$Image("Oxford/MAP/accessibility_to_cities_2015_v1_0")$
  toInt()
gpw <- paste0("CIESIN/GPWv411/GPW_UNWPP-Adjusted_Population_Density/",
              "gpw_v4_population_density_adjusted_to_2015_", 
              "unwpp_country_totals_rev11_2000_30_sec")

pop_dens <- ee$Image(gpw)$
  select("unwpp-adjusted_population_density")$
  focal_mean(radius = 20e3, kernelType = "circle", 
                                units = "meters")$
  toInt()

countries <- ee$FeatureCollection("USDOS/LSIB_SIMPLE/2017")
countries_img <- countries$
  filter(ee$Filter$notNull(list("ADM0_CODE")))$
  reduceToImage(properties = list("ADM0_CODE"), reducer = ee$Reducer$first())

biome_img <- select_ecoregs$
  filter(ee$Filter$notNull(list("BIOME_NUM")))$
  reduceToImage(properties = list("BIOME_NUM"), reducer = ee$Reducer$first())

wdpa <- ee$FeatureCollection("WCMC/WDPA/current/polygons")$
  set("PA", 1)

wdpa_buffer <- wdpa.buffer(10000)$ # 10km buffer
  set("PA_buffer", 1)

wdpa_yr <- wdpa$
  filter(ee$Filter$notNull(list("STATUS_YR")))$
  reduceToImage(properties = list("STATUS_YR"), reducer = ee$Reducer$first())

# stack bands into a single raster and export ----------------------------------
combined <- cover$addBands(c(lossyear, loss, elev, slope, travel_time, 
                             pop_dens, countries_img, biomes_img, wdpa, wdpa_yr, 
                             wdpa_buffer))$
  rename(c("cover", "lossyear", "loss", "elev", "slope", "travel_time", 
           "pop_dens", "country", "biome", "PA", "PA_status_yr", "PA_buffer"))
```

# Add countries and PAs

```{r}





#select_countries <- countries$filterBounds(select_ecoregs$union())
countries_download <- ee_as_sf(countries, 
                               dsn = "gee/countries.geojson", 
                               container = "rgee", via = "drive", lazy = TRUE)
ecoregions_download <- ee_as_sf(select_ecoregs, 
                                dsn = "gee/select_ecoregs.geojson", 
                                container = "rgee", via = "drive", lazy = TRUE)
ee_utils_future_value(countries_download)
ee_utils_future_value(ecoregions_download)
countries <- st_read("gee/countries.geojson")
my_ecoregions <- st_read("gee/select_ecoregs.geojson")




my_biomes <- my_ecoregions %>%
  st_make_valid() %>%
  group_by(BIOME_NAME, BIOME_NUM) %>%
  summarise()


table_biomes <- my_ecoregions %>%
  st_drop_geometry() %>%
  select(BIOME_NAME, BIOME_NUM) %>%
  unique()

# define areas of interest my areas of interest
aois <- countries %>%
  st_intersection(my_ecoregions) 

aois %>%
  tm_shape() + 
  tm_polygons(col = "BIOME_NAME")

dir.create("gee/milestones")
write_geoparquet(aois, "gee/milestones/aois.parquet")




aois_tbl <-aois %>%
  st_drop_geometry() %>%
  select(id, country_co, country_na, BIOME_NUM, BIOME_NAME) %>%
  mutate(id = as.numeric(id)) %>%
  unique()


clip_and_download <- function(aoi) {
  focus <- combined$updateMask(country = country_id)$
    updateMask(biome = biome_id)
  print(paste("Process", aoi["BIOME_NAME"], "for", aoi["country_na"], 
              paste(i, aois_num, sep = "/")))
  ee_as_raster(focus, via = "drive", container = "rgee", lazy = TRUE,
               dsn = paste0("rgee/", aoi["country_co"], "-", aoi["BIOME_NUM"], 
                            ".tif"),
               crsTransform = paste(scale, 0, -180, 0, -scale, 90, sep = ", "),
               crs = "EPSG:4326",
               dimensions = paste(as.character(round(360/scale)), "x",
                                  as.character(round(360/scale))),
               maxPixels = 1e13)
}

aois_num <- nrow(aois_tbl)
all_runs <- list.dirs()
for (i in 1:aois_num) {
  aoi <- slice(aois)
  print(paste(i, aois_num, sep = "/"))
  this_run <- clip_and_down(aoi)
  all_runs <- c(all_runs, this_run)
}
```

# Load deforestation drivers

```{r}

# Drivers

wwf_ecoregions %>%
  st_drop_geometry() %>%
  group_by(BIOME) %>%
  summarise(n_distinct = n_distinct(BIOME),
            n = n())
wwf_ecoregions %>%
  st_drop_geometry() %>%
  group_by(ECO_NAME) %>%
  summarise(n_distinct = n_distinct(ECO_NAME),
            n = n())

```

Load GEE data

```{r}


```
