---
title: "Reimplement matching"
editor: visual
editor_options: 
  chunk_output_type: console
---

## Trying a different computation strategy

We have not yet been able to successfully replicate Wolf et al. (2021) as their code does not run successfully and generates errors. We identified the origin of some errors and corrected them, but we remained stuck with Julia errors. We tried different computing environment : local, cloud servers and with several different configurations (Windows, Linux, different R, python and Julia versions, etc.), and with large amounts of CPUs and memory.

We believe that the computing sequence elaborated by this study authors is particularly complex because of the very large amount of data to be processed. We propose a different approach that seems more straight forward and use the same entry parameters as Wolf et al. (2021).

Here we attempt to reproduce every analysis steps and parameters of Wolf et al. (2021) but to implement a different data processing strategy that:

-   implies less manipulations steps to be less error prone;
-   only relies on one language and configuration for local processing.
-   relies on cloud data processing for very big datasets: we leverage the cloud computing platform used by the authors to download their source data (Google earth engine) to do more than only data acquisition: we use it also to preprocess and combine the source data before download;;
-   takes advantage that the matching algorithm segments by country and biome to process the data in smaller batches that are easier to process on commonly accessible processing configurations.
-   containerize and archive our computing environement to enable any future researcher to reproduce it in the future.

```{r}
setwd("replication_report")
library(tidyverse)
library(sf)
library(tmap)
library(geoarrow)
library(geodata)
library(units)
library(terra)
library(stars)
```

## Fetch and prepare data from other sources than GEE

We gather data for national boundaries and biomes and combine it.

```{r}
#| fig-cap: "Areas of interest (biome segmented along national boundaries)"

# Countries  using geodata (ie. GADM), rnaturalearth is broken
dir.create("revamp")
dir.create("revamp/countries")

countries <- world(resolution=2, level=0, path = "revamp/countries") %>%
  st_as_sf() %>%
  st_make_valid() %>%
  mutate(country_num = 1:nrow(.), .before = GID_0)

# We prepare biome polygons (only at first run, if not already done)
if (!file.exists("revamp/biomes/biomes.parquet")) {
  # Fetch biomes
  dir.create("revamp/biomes")
  if (!file.exists("revamp/biomes/wwf_biomes.zip")) {
    wwf_url <- paste0("https://files.worldwildlife.org/wwfcmsprod/files/", 
                      "Publication/file/6kcchn7e3u_official_teow.zip")
    download.file(wwf_url, "revamp/biomes/wwf_biomes.zip")
    unzip("revamp/biomes/wwf_biomes.zip", exdir = "revamp/biomes")
  }
  # Merge ecoregions with the same biome
  biomes <- st_read("revamp/biomes/official/wwf_terr_ecos.shp") %>%
    st_make_valid() %>%
    group_by(BIOME) %>%
    summarise()
  write_geoparquet(biomes, "revamp/biomes/biomes.parquet")
} else {
  read_geoparquet("revamp/biomes/biomes.parquet")
}

# Rename according to documentation and filter
biomes <- biomes %>%
  rename(biome_num = BIOME) %>%
  mutate(biome_name = case_when(
    biome_num == 1 ~ "Tropical & Subtropical Moist Broadleaf Forests",
    biome_num == 2 ~ "Tropical & Subtropical Dry Broadleaf Forests",
    biome_num == 3 ~ "Tropical & Subtropical Coniferous Forests",
    biome_num == 4 ~ "Temperate Broadleaf & Mixed Forests",
    biome_num == 5 ~ "Temperate Conifer Forests",
    biome_num == 6 ~ "Boreal Forests/Taiga",
    biome_num == 7 ~ "Tropical & Subtropical Grasslands, Savannas & Shrublands",
    biome_num == 8 ~ "Temperate Grasslands, Savannas & Shrublands",
    biome_num == 9 ~ "Flooded Grasslands & Savannas",
    biome_num == 10 ~ "Montane Grasslands & Shrublands",
    biome_num == 11 ~ "Tundra",
    biome_num == 12 ~ "Mediterranean Forests, Woodlands & Scrub",
    biome_num == 13 ~ "Deserts & Xeric Shrublands",
    biome_num == 14 ~ "Mangroves",
    .default = "Unknown"), .before = geometry)
my_biomes <- biomes %>%
  filter(biome_name %in% c("Tropical & Subtropical Moist Broadleaf Forests",
                          "Tropical & Subtropical Dry Broadleaf Forests",
                          "Tropical & Subtropical Coniferous Forests",
                          "Temperate Broadleaf & Mixed Forests",
                          "Temperate Conifer Forests",
                          "Boreal Forests/Taiga",
                          "Mangroves")) %>%
  mutate(biome_num = case_when(
    biome_name == "Tropical & Subtropical Moist Broadleaf Forests" ~ 0,
    biome_name == "Tropical & Subtropical Dry Broadleaf Forests" ~ 1,
    biome_name == "Tropical & Subtropical Coniferous Forests" ~ 2,
    biome_name == "Temperate Broadleaf & Mixed Forests" ~ 3,
    biome_name == "Temperate Conifer Forests" ~ 4,
    biome_name == "Boreal Forests/Taiga" ~ 5,
    biome_name == "Mangroves" ~ 6))

# Areas of interest (AOIs) combine countries and biomes
aois <- st_intersection(countries, my_biomes) %>%
  mutate(area = st_area(.)) %>%
  arrange(area) # to start with the smallest for testing

aois %>%
  rename(Biome = biome_name) %>%
  tm_shape() +
  tm_polygons(col = "Biome") + 
  tm_borders() + 
  tm_layout(legend.outside = TRUE,
            legend.outside.position = "right")
```

We obtain `r nrow(aois)` polygons of country/biomes combinations.

We also download the data from [@curtis2018] on deforestation drivers, which is a raster data.

```{r}
# We only execute this once
if (!file.exists("revamp/drivers/curtis_et_al_orig.tif")) {
  science_url <- paste0("https://www.science.org/action/downloadSupplement?",
                        "doi=10.1126%2Fscience.aau3445&file=aau3445-data-s3.tif")
  download.file(url = science_url, 
                destfile = "revamp/drivers/curtis_et_al_orig.tif",
                method = "curl")
  # load country + biome data
drivers <- rast("revamp/drivers/curtis_et_al_orig.tif") %>%
  project("epsg:4326") %>%
  writeRaster("revamp/drivers/drivers_curtis.tif")
}
```

## Move external data to GEE and prepare all information there

The first step of the data analysis workflow of Wolf et al. consisted in fetching data from Google Earth Engine. They downloaded each dataset (forest cover, forest loss, forest gain, elevation, slope, travel time and population density) as separate raster files to process them locally on their own computer. Because of the massive amount of such data, the subsequent steps in their analysis workflow are complex, rely from different languages, a large number of libraries (several of them now deprectated), fails to run and we are unable to debug it.

The Google Earth Engine can be leveraged to do much more than only downloading data, and therefore we plan to re-implement on this cloud environment the data preparation steps, instead of doing it locally on individual machines.

We use the R package `rgee` as an interface to Google Earth engine API [@aybar2020].

```{r}
# Sometime rgee does not find the right python env, so we specify it
reticulate::use_python("C:/Users/fbede/AppData/Local/r-miniconda/envs/rgee")
library(rgee) # accesses GEE through its python API
ee_Initialize(gsc = TRUE)
dir.create("revamp/gee")
```

We start by uploading the country, biome and deforestation drivers' data prepared in the previous step.

```{r}
#| eval: false

# We add this manually, as we had issues with API auth. protocol for GSC
drivers_curtis <- ee$Image("projects/ee-fbedecarrats/assets/drivers_curtis")
# We also load the AOIs manually due to the same problem
if (!file.exists("revamp/aois/aois_shp.zip")) {
  dir.create("revamp/aois")
  st_write(aois, "revamp/aois/aois.shp")
  aoi_files = list.files(path = "revamp/aois", pattern = "aois.*", 
                         full.names = TRUE)
  zip(zipfile = "revamp/aois/aois_shp.zip", files = aoi_files)
}
aois_ee <- ee$FeatureCollection("projects/ee-fbedecarrats/assets/aois")
# We get the colors used by R using tmaptools::palette_explorer()
biome_fills <- ee$Image()$byte()$
  paint(featureCollection = aois_ee,
        color = "biome_num")
biome_nation_borders <- ee$Image()$byte()$
  paint(featureCollection = aois_ee,
        color = 1,
        width = 1)

r_map_palette <- c("b3de69", # light green: Tropical & Subtropical Moist Broadleaf Forests 
                   "fdb462", # light orange: Tropical & Subtropical Dry Broadleaf Forests
                   "80b1d3", # blue: Tropical & Subtropical Coniferous Forest
                   "bebada", # purple: Temperate Broadleaf & Mixed Forests
                   "fb8072", # dark orange: Temperate Conifer Forests
                   "8dd3c7", # turquois: Boreal Forests/Taiga
                   "ffffb3") # yellow: Mangroves
Map$addLayer(biome_fills, list(min = 0, max = 6, palette = r_map_palette)) +
  Map$addLayer(biome_nation_borders, list(palette = "000000"))
```

![Diplay of biomes segmented by national boarders on google earth engine](img/biomes_national_rgee.png) \# Clip Hansen data with biome polygons

Instead of downloading and processing all the GFC data like Wolf and al., we instead keep only the data located within the perimeter of the biomes of interest.

```{r}
scale <- 0.008983153 # From Wolf et al. 2021
drive_folder <- "rgee"
local_folder <- "temp"

# Hansen/GFC -------------------------------------------------------------------
gfc <- ee$Image("UMD/hansen/global_forest_change_2018_v1_6")

# We save the output as image to display it in quarto
gfc_mask <- gfc$select("datamask")$eq(1)
gfc_wolf <-  gfc$updateMask(gfc_mask)
Map$addLayer(gfc_wolf, list(
  bands = c("loss", "treecover2000", "gain"),
  max = c(1, 255, 1)))
```

![GFC data exported by wolf to be processed on a local computer (green: tree cover in 2000, red: tree cover loss 2001-2018, blue: tree cover gain 2001-2012)](img/gfc_data_exported_by_wolf.png)
```{r}
# We save the output as image to display it in quarto
gfc_aois <- gfc_wolf$clipToCollection(aois_ee)
Map$addLayer(gfc_aois, list(
  bands = c("loss", "treecover2000", "gain"),
  max = c(1, 255, 1)))

```
![GFC data restricted to areas of sudy to export for matching (green: tree cover in 2000, red: tree cover loss 2001-2018, blue: tree cover gain 2001-2012)](img/gfc_data_on_aois.png)
# Stack information as bands in a single raster

Wolf et al. downloaded each information source in a separate raster, that they later on joined. We find easier and less error-prone to stack all the information as bands of the same raster. The data might be downloaded in separate files for different national segment of each biome, but for the same regions, all the complementary information is attached to the same pixels with the same resolution (alias, a "clean" table).

```{r}
# We reference other data sources that Wolf et al. fetched from GEE
elev <- ee$Image("USGS/GTOPO30")$toInt() # Elevation
slope <- ee$Terrain$slope(elev)$toInt() # Slope
# Travel time
travel_time <- ee$Image("Oxford/MAP/accessibility_to_cities_2015_v1_0")$
  toInt()
# Population density
gpw <- paste0("CIESIN/GPWv411/GPW_UNWPP-Adjusted_Population_Density/",
              "gpw_v4_population_density_adjusted_to_2015_", 
              "unwpp_country_totals_rev11_2000_30_sec")
pop_dens <- ee$Image(gpw)$
  select("unwpp-adjusted_population_density")$
  focal_mean(radius = 20e3, kernelType = "circle", 
                                units = "meters")$toInt()
# We add information on protected areas
wdpa <- ee$FeatureCollection("WCMC/WDPA/current/polygons")$
  set("PA", 1)
wdpa_buffer <- wdpa$geometry()$buffer(10000)
wdpa_buffer <- ee$Feature(wdpa_buffer)$
  set("PA_buffer", 1)
wdpa_yr <- wdpa$
  filter(ee$Filter$notNull(list("STATUS_YR")))$
  reduceToImage(properties = list("STATUS_YR"), reducer = ee$Reducer$first())

# We add countries and biomes
country_img <- countries$
  reduceToImage(properties = list("country_num"), reducer = ee$Reducer$first())
biome_img <- select_ecoregs$
  reduceToImage(properties = list("biome_num"), reducer = ee$Reducer$first())

# stack bands into a single raster and export ----------------------------------
combined <- gfc_aois$select(c("treecover2000", "loss", "lossyear"))$
  addBands(c(elev, slope, travel_time, pop_dens, # fetched from GEE by Wolf
             wdpa, wdpa_yr, wdpa_buffer, # also from GEE
             countries_img, biomes_img))$ # prepared in R and added to GEE
  rename(c("cover", "loss", "lossyear", 
           "elev", "slope", "travel_time", "pop_dens",
           "PA", "PA_status_yr", "PA_buffer",
           "country", "biome"))
```

The previous code creates a raster with the following bands: cover, loss, lossyear, elev, slope, travel_time, pop_dens, PA, PA_status_yr, PA_buffer, country, biome.


Now we will iterate to download this raster in separated file, with one file per national segment of each biome of interest.




```{r}
# Test
wdpa_mdg <- ee$FeatureCollection("WCMC/WDPA/current/polygons")$
  filter("ISO3 == 'MDG'")
wdpa_mdg_buffer <- wdpa_mdg$geometry()$buffer(10000)
wdpa_mdg_buffer <- ee$Feature(wdpa_mdg_buffer)$
  set("PA_buffer", 1)
ee_print(wdpa_mdg_buffer)

```


# Add countries and PAs

```{r}





#select_countries <- countries$filterBounds(select_ecoregs$union())
countries_download <- ee_as_sf(countries, 
                               dsn = "gee/countries.geojson", 
                               container = "rgee", via = "drive", lazy = TRUE)
ecoregions_download <- ee_as_sf(select_ecoregs, 
                                dsn = "gee/select_ecoregs.geojson", 
                                container = "rgee", via = "drive", lazy = TRUE)
ee_utils_future_value(countries_download)
ee_utils_future_value(ecoregions_download)
countries <- st_read("gee/countries.geojson")
my_ecoregions <- st_read("gee/select_ecoregs.geojson")




my_biomes <- my_ecoregions %>%
  st_make_valid() %>%
  group_by(BIOME_NAME, BIOME_NUM) %>%
  summarise()


table_biomes <- my_ecoregions %>%
  st_drop_geometry() %>%
  select(BIOME_NAME, BIOME_NUM) %>%
  unique()

# define areas of interest my areas of interest
aois <- countries %>%
  st_intersection(my_ecoregions) 

aois %>%
  tm_shape() + 
  tm_polygons(col = "BIOME_NAME")

dir.create("gee/milestones")
write_geoparquet(aois, "gee/milestones/aois.parquet")




aois_tbl <-aois %>%
  st_drop_geometry() %>%
  select(id, country_co, country_na, BIOME_NUM, BIOME_NAME) %>%
  mutate(id = as.numeric(id)) %>%
  unique()


clip_and_download <- function(aoi) {
  focus <- combined$updateMask(country = country_id)$
    updateMask(biome = biome_id)
  print(paste("Process", aoi["BIOME_NAME"], "for", aoi["country_na"], 
              paste(i, aois_num, sep = "/")))
  ee_as_raster(focus, via = "drive", container = "rgee", lazy = TRUE,
               dsn = paste0("rgee/", aoi["country_co"], "-", aoi["BIOME_NUM"], 
                            ".tif"),
               crsTransform = paste(scale, 0, -180, 0, -scale, 90, sep = ", "),
               crs = "EPSG:4326",
               dimensions = paste(as.character(round(360/scale)), "x",
                                  as.character(round(360/scale))),
               maxPixels = 1e13)
}

aois_num <- nrow(aois_tbl)
all_runs <- list.dirs()
for (i in 1:aois_num) {
  aoi <- slice(aois)
  print(paste(i, aois_num, sep = "/"))
  this_run <- clip_and_down(aoi)
  all_runs <- c(all_runs, this_run)
}
```

# Load deforestation drivers

```{r}

# Drivers

wwf_ecoregions %>%
  st_drop_geometry() %>%
  group_by(BIOME) %>%
  summarise(n_distinct = n_distinct(BIOME),
            n = n())
wwf_ecoregions %>%
  st_drop_geometry() %>%
  group_by(ECO_NAME) %>%
  summarise(n_distinct = n_distinct(ECO_NAME),
            n = n())

```

Load GEE data

```{r}


```
