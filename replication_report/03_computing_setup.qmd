---
title: "Testing multilingual setup"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

The code used by Wolf and colleagues to obtain their results and published on Github mixes three different coding languages: python, R and Julia. Moreover, although the code is generally clearly written and includes some explanatory comments, several adjustments in the code are required to enable it to run on a different machine. This document provides a technical procedure to enable the reproducibility of the computations shared by Wolf and colleagues.

## Comments on the reproducibility of the analyses.

Some good practices are not respected. The analysis mentions that the sources are publicly available, but the data is not available as is: needs an access to Google Earth Engine. The API is not public and this accessibility is conditionned by the will of Google to keep its platform in free access and not to make any evolution in the GEE API that would not be backward compatible. For these resons, it seems recommended to archive the data outputed by sources which reproducibility is not guaranteed.

It is also very likely that some data used for the analysis gets updated: UICN species data is regularly updated WDPA data on protected area also

The obtaining of the data is not documented for BOTW and IUCN range maps.

## Technical environment

It is possible to combine different programming languages in statistical computing environments such as Jupyter or RMarkdown, or its new generation Quarto. We decided to use Quarto because of its versatility and our familiarity of this tool.

Quarto can be obtained at www.quarto.org.

Requires Linux or a Windows machine running WSL, otherwise a strong rewriting of some files is needed. We run it on a Windows personal computer.

## Prerequisites

### Google services

Have a gmail account to access google services Google Earth Engine and Google Drive.

### wget

The code requires a working installation of Wget. Wget comes with linux platform and can be installed on MAC. On Windows, it requires Windows Subsystem for Linux.

## Set up R

We install all R dependencies that might be required

```{r}

# Install from Github --------------------------------------------------------

# Some packages need to be installed from developper sources because there are
# not available or official sources have some issues.
# Installing version 1.3.5 which is the last working version apparently
remotes::install_github("https://github.com/cran/doMC/tree/fbea362b96cc4469deb6065ff9fbd5d4794ccac1")
remotes::install_github("https://github.com/cran/gdalUtils")
remotes::install_github("jonocarroll/ggeasy")
remotes::install_github("https://github.com/hunzikp/velox", upgrade = FALSE) 


# Install from CRAN ------------------------------------------------------------

# These packages are available from the usual source from R
required_packages <- c( # List all required R packages
  "reticulate", # To interact with python (normally installed with Quarto)
  "JuliaCall", # To interact with Julia 
  "tidyverse", # To facilitate data manipulation
  # All packages below are used in Wolf and al. code files:
  "countrycode",
  "cowplot",
  "data.table",
  "dtplyr",
  "fasterize",
  "foreach",
  "foreign",
  "ggforce",
  "ggplot2",
  "ggrepel",
  "GpGp",
  "grid",
  "jsonlite",
  "landscapetools",
  "lme4",
  "MCMC.OTU",
  "ncdf4",
  "parallel",
  "pbapply",
  "plyr",
  "raster",
  "rasterVis",
  "rbounds",
  "RColorBrewer",
  "RCurl",
  "readr",
  "reshape2",
  "rgdal",
  "rjson",
  "rnaturalearth",
  "scales",
  "sf",
  "smoothr",
  "spaMM",
  "spgwr",
  "spmoran",
  "spNNGP",
  "stars",
  "stringr",
  "tidyverse",
  "unix",
  "velox",
  "viridis",
  "wbstats",
  "wdpar") 
missing <- !(required_packages %in% installed.packages())

# Install 
if(any(missing)) install.packages(required_packages[missing])
```

## Get the data

```{r}

library(dplyr)
library(stringr)
library(aws.s3)
library(purrr)
library(stringr)

save_object(
  file = "other.zip",
  object = "Replication_wolf/other2.zip",
  bucket = "fbedecarrats",
  region = "")

# Securely getting the data from the S3 bucket (private and encrypted)
system("vault kv get -format=table onyxia-kv/fbedecarrats/species_info", 
       intern = TRUE) %>%
  pluck(17) %>%
  str_extract("[:alnum:]*$") -> my_key
system(paste("unzip -P", my_key, "other.zip"))
file.remove("other.zip")

# Listing files in bucket
my_files <- get_bucket_df(bucket = "fbedecarrats",
                          prefix = "Replication_wolf",
                          region = "")
# Selecting tifs at root
my_tifs <- my_files %>%
  filter(str_ends(Key, ".tif")) %>%
  pluck("Key")

# Generating the destination paths
my_tifs_dest <- my_tifs %>%
  str_replace("Replication_wolf/data_processed", "data_input") %>%
  str_replace("Replication_wolf", "data_processed")

# A function to iterate/vectorize copy
save_from_s3 <- function(x, y) {
  aws.s3::save_object(
    object = x,
    bucket = "fbedecarrats",
    file = y,
    overwrite = FALSE,
    region = "")
  }
# copy from S3 to local
map2(my_tifs, my_tifs_dest, save_from_s3)

# Select PA shp
my_PAs <- my_files %>%
  filter(str_detect(Key, "/PAs/WDPA")) %>%
  pluck("Key")
# Create paths in local machine
my_PAs_dest <- my_PAs %>%
  str_replace("Replication_wolf/", "data_input/")
# Retrieve the data
map2(my_PAs, my_PAs_dest, save_from_s3)


# Select WWF shp
my_biomes <- my_files %>%
  filter(str_detect(Key, "/official/")) %>%
  pluck("Key")
# Create paths in local machine
my_biomes_dest <- my_biomes %>%
  str_replace("Replication_wolf/", "data_input/")
# Retrieve the data
map2(my_biomes, my_biomes_dest, save_from_s3)

# # Create a function to put data from local machine to S3
# put_to_s3 <- function(x, y) {
#   aws.s3::put_object(
#     file = x,
#     object = y,
#     bucket = "fbedecarrats",
#     region = "",
#     multipart = TRUE)
# }
# 
# # Apply it to the PAs
# wdpa_local_files <- list.files("data_input/PAs", full.names = TRUE)
# wdpa_s3_objects <- wdpa_local_files %>%
#   str_replace("data_input", "Replication_wolf")
# map2(wdpa_local_files, wdpa_s3_objects, put_to_s3)
# # Also to the source
# put_to_s3("data_input/WDPA_Jan2023_Public.gdb.zip", 
#           "Replication_wolf/WDPA_Jan2023_Public.gdb.zip")
```

get the right WDPA data. Wolf et al. use the data from January 2020.

```{r}
#| eval: false

library(tidyverse)
library(wdpar)
library(sf)
if (!dir.exists("data_input/PAs")) {
  # Get the data from protectedplanet
  wdpa_global <- wdpa_fetch("global", download_dir = "data_input")
  # Build a name with same structure than Wolf et al.
  wdpa_file_name <- list.files("data_input", pattern = ".gdb.zip") %>%
    str_remove("_Public.gdb.zip") %>%
    paste0("data_input/PAs/", ., "-shapefile-polygons.shp")
  # Filter for polygons (filtering out points to be sure not to exclude something
  # by mistake) and write locally.
  wdpa_global %>%
    filter(st_geometry_type(.) != "MULTIPOINT") %>%
    st_write(dsn = wdpa_file_name)
}

wdpa_global %>%
  #"We excluded PAs from our primary analysis that: 
  #[1] had point (centroid) information only, 
  filter(st_geometry_type(.) != "MULTIPOINT") %>%
  #[2] were exclusively marine, 
  filter(MARINE != "2") %>% #0: 100% terrestrial, 1: both, 2: 100% marine
  #[3] were established after 2000 since the forest loss data range from 2001 to 2018,
  filter(STATUS_YR > 2000) %>%
  #[4] had area less than 1 km2 since forest change in small PAs can be hard to
  # estimate accurately, 
  filter(REP_AREA - REP_M_AREA >= 1) %>%
  st_write(dsn = wdpa_file_name)

# 48066 PAs filtered. Check as this seems quite low compared to 286356 in the 
# WDPA global database.
```

Get WWF ecoregions

```{r}
my_url <- "https://files.worldwildlife.org/wwfcmsprod/files/Publication/file/6kcchn7e3u_official_teow.zip"
download.file(my_url, destfile = "temp/teow.zip")
unzip(zipfile = "temp/teow.zip", exdir = "data_input")
```

## Set up python

```{r}
library(reticulate) # to run python from R

# Variables to modify
my_envname <- "replication-wolf"
scripts_to_run <- "003" # or c("001", "003") or "001"

# Install python if not already present
if (!dir.exists(miniconda_path())) {
  install_miniconda()
}
# Create environment if not already
if (!my_envname %in% conda_list()$name) {
  conda_create(my_envname)
}
# Packages needed for each script
requirements <- list(
  "001" = c("earthengine-api", "rasterio", "pandas", "pydrive"),
  "003" = c("fiona", "rasterio", "ray-default", "dbfread", "pandas"))
addition <- c()
# Combined depending on the variable defined at beginning of code chunk
required <- unique(unlist(requirements[scripts_to_run]))
# identify which ones are missing
missing_packages <- required[!required %in% py_list_packages(my_envname)$package]
# Install those
conda_install(envname = my_envname, 
              packages = missing_packages)
conda_install(envname = my_envname, 
              packages = "libstdcxx-ng")
# Activate the corresponding environment
use_condaenv(my_envname)
```

Create an authorization

```{python}
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive

gauth = GoogleAuth()
# Try to load saved client credentials
gauth.LoadCredentialsFile("mycreds.txt")
if gauth.credentials is None:
    # Authenticate if they're not there
    gauth.LocalWebserverAuth()
elif gauth.access_token_expired:
    # Refresh them if expired
    gauth.Refresh()
else:
    # Initialize the saved creds
    gauth.Authorize()
# Save the current credentials to a file
gauth.SaveCredentialsFile("mycreds.txt")
```

## Set up Julia

First we install or set-up Julia from R if needed.

```{r}
library(JuliaCall)
# Install or set up julia if needed
julia_setup()
```

We also install the packages used in the code of Wolf et al.

```{julia}
using Pkg
Pkg.add("ArchGDAL")
Pkg.add("DataFrames")
Pkg.add("Discretizers")
Pkg.add("Shapefile")
Pkg.add("FreqTables")
Pkg.add("Plots")
Pkg.add("StatsBase")
Pkg.add("CSV")
Pkg.add("LibGEOS")
```

## Harmonize paths

### Make code paths generic

```{r}
library(tidyverse)

replace_all <- function(x, pattern, replacement) {
  print(x)
  readLines(x) %>%
    str_replace_all(pattern, replacement) %>%
    writeLines(x)
}

# Replace working directory or data sources paths
list.files(pattern = "[0-9]{3}.*") %>%
  map(replace_all, "/home/chrisgraywolf/analysis_desktop/", "")
list.files(pattern = "[0-9]{3}.*") %>%
  map(replace_all, "/home/chrisgraywolf/shared/analysis/PA_matching", ".")
```

### Add required subfolders

```{r}
dir.create("data_input")
dir.create("data_input/PAs")
dir.create("temp")
dir.create("data_processed")
dir.create("data_processed/rasters")
```

## Adapt to Windows system (optionnal)

Install `parallel`: On Windows, run the command `wsl sudo apt-get update` followed by the command `wsl sudo apt-get install parallel`. On Linux, run the same commands without `wsl`, that is run the command `sudo apt-get update` followed by the command `sudo apt-get install parallel`.

```{r}
# Replace all wget calls by wsl wget
replace_all("002 - dl_IUCN.R", "wget", "wsl wget")

```

## Testing R

```{r}
1 + 1

```

## Testing python

Needs reticulate (already installed here).

```{python}
1 + 1
```

## Testing Julia

## Prepare data

The data from IUCN now comes in 2 parts for the reptiles.

```{r}
library(tidyverse)
library(sf)

reptiles1 <- st_read("data_input/range_maps/REPTILES_PART1.shp") 
  
reptiles1 %>%
  select(-OBJECTID) %>%
  st_write(dsn = "data_input/range_maps/REPTILES_PART2.shp", append = TRUE)

file.rename(list.files(path = "data_input/range_maps", pattern ="REPTILES_PART2",
                       full.names = TRUE),
            str_replace(list.files(path = "data_input/range_maps",
                                   pattern="REPTILES_PART2", full.names = TRUE), 
                        pattern="REPTILES_PART2", "REPTILES"))
```

## 

## Executing 004 - join rasters

\# The call below throws errors messages during execution:

\# Error in x\$.self\$finalize() : attempt to apply non-function

\# According to {raster}

\# https://github.com/rspatial/raster/issues/282

URL for Curtis et al. was: https://science.sciencemag.org/highwire/filestream/715492/field_highwire_adjunct_files/2/aau3445-Data-S3.tif

It is now: https://www.science.org/action/downloadSupplement?doi=10.1126%2Fscience.aau3445&file=aau3445-data-s3.tif

The name of the file has been changed to lowercase: data_input/aau3445-Data-S3.tif becomes aau3445-data-s3.tif
