[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Replication of Wolf et al. (2021) “A forest loss report card for the world’s protected areas”",
    "section": "",
    "text": "1 Summary\nThis document (hereafter refered to as “the replication study”) provides a technical description of the process we are following to reproduce and replicate the work titled “A forest loss report card for the world’s protected areas” published in Nature Ecology & Evolution by Wolf, Levi, Ripple, Zárrate-Charry and Betts (2021, hereafter referred to as “the original study”). Before starting any analysis, we specified the objectives of this exercises and the different steps we intend to follow in Chapter 2. Our intents to create a computing environment able to run the original study’s code is presented in Chapter 3. The code published alongside the original study automatically fetches a subset of the data required by the statistical computation, but some sources links need to be updated. Chapter 4 describes these updates and how to obtain the data that is missing when using only the published code. Chapter 5 describes the subsequent steps followed to execute the code, including the first coding errors that we found. The code for the matching algorithm fails to run, we are not able to debug it and the corresponding author of the original study is not available to debug it. We therefore decided to re-implement the data analysis process described in the original study but in a way that could be more straightforward, less error-prone and reproducible. We describe these intents in Chapter 7. We describe some issues we identified in the analysis process of the original study in Chapter 8.\n\n\n\n\nWolf, Christopher, Taal Levi, William J. Ripple, Diego A. Zárrate-Charry, and Matthew G. Betts. 2021. “A Forest Loss Report Card for the World’s Protected Areas.” Nature Ecology & Evolution 5 (4): 520–29. https://doi.org/10.1038/s41559-021-01389-0."
  },
  {
    "objectID": "replication_report/02_preanalysis_plan.html",
    "href": "replication_report/02_preanalysis_plan.html",
    "title": "2  Pre-analysis plan for the replication of Wolf et al. (2021)",
    "section": "",
    "text": "We intend to run a replication of the paper published by Wolf and colleagues in Nature Ecology & Evolution in 2021 (Wolf et al. 2021). In this document, we lay out with the analysis steps we plan to carry out before starting any analysis. This document was first registered in Github on the 16/12/2022, as can be verified on its modification history page. This has two main objectives:\n\nhelp the replication authors to agree with anticipation on the study process; and\ndemonstrate to the journal reviewers, original study authors and future readers that the replication authors did not engage in any specification searching behavior.\n\nFollowing scientific good practices, we pre-specify the replication procedure we anticipate applying and we register this pre-specification in Github, a platform ensuring the traceability of documents, code and data that is commonly used, among other areas, in econometrics and geospatial analysis (Vitolo et al. 2015). This submission is at the very beginning of the replication process, that is, before running any actual data analysis. These are the step we plan to follow:\n\nReproduce the analysis implemented by Wolf et al. on the 2001-2018 GFC data and validate that we obtain the same result. This aims at verifying that the published analytical procedure indeed produces the published results.\nReproduce the analysis implemented by Wolf et al. on the 2001-2018 GFC, replacing in one country (Madagascar) the WDPA data by another, more reliable source, and verify if the results still hold for this country. We noticed in a previous work on Madagascar that the WDPA database is incomplete and includes inaccurate geometries and attributes for several protected areas (Bédécarrats et al. 2022). We believe that errors or missing information in the data reported to WDPA might be correlated with the protected areas management quality and, therefore, with protected areas outcomes such as deforestation. This test aims at verifying the use of a source lacking reliability on the treatment (such as WDPA in the case of Madagascar) might have lead the researchers to come to biased conclusions.\nReproduce the analysis implemented by Wolf et al. with the most recent available GFC data (2001-2021, or 2001-2022 if available). This aims at verifying that the effects observed by Wolf et al. remain valid after the study period.\nReproduce the analysis implemented by Wolf et al., replacing the 2001-2018 GFC by the JRC tropical moist forest dataset. The TMF dataset is allegedly more reliable: it was developed 8 years after the first disclosure of the GFC data and provides some enhancements: it covers a larger period of time (starting in 1990) and it differentiates, within forest cover loss, between deforestation and forest degradation.\n\nThis document might be updated after its initial registry on Github. Every detail of any subsequent modification will be systematically and automatically recorded and time-stamped on Github, enabling anyone to access the detailed history of modification.\n\n\n\n\nBédécarrats, Florent, Marc Bouvier, Jeanne de Montalembert, and Marin Ferry. 2022. “Aires Protégées.” In, IRD. Tutoriel de Formation. Tuléar. https://fbedecarrats.github.io/conservation-deforestation-madagascar/01-aires_protegees.html.\n\n\nVitolo, Claudia, Yehia Elkhatib, Dominik Reusser, Christopher J. A. Macleod, and Wouter Buytaert. 2015. “Web Technologies for Environmental Big Data.” Environmental Modelling & Software 63 (January): 185–98. https://doi.org/10.1016/j.envsoft.2014.10.007.\n\n\nWolf, Christopher, Taal Levi, William J. Ripple, Diego A. Zárrate-Charry, and Matthew G. Betts. 2021. “A Forest Loss Report Card for the World’s Protected Areas.” Nature Ecology & Evolution 5 (4): 520–29. https://doi.org/10.1038/s41559-021-01389-0."
  },
  {
    "objectID": "replication_report/03_computing_setup.html#procedure",
    "href": "replication_report/03_computing_setup.html#procedure",
    "title": "3  Computing environment setup",
    "section": "3.1 Procedure",
    "text": "3.1 Procedure\nGuidelines by scientific journals to promote the reproducibility of published scientific articles recommend the authors to publish their code including a file named README that specifies the configuration of the computing environment and the procedure to follow to obtain the same results (for guidance on code publications, see here for social science journals or here for Nature journals. This source code only includes an empty README file, so we made several attempts to guess a working configuration. We tested the code on the following platforms:\n\nA Windows personal computer with 32 Go RAM and a 8 cores processor: the file 001 - dl_GEE.pysuccessfully ran, but the subsequent code files failed, because they call on Linux commands (e.g. wget) or rely on libraries that seem to be unix native.\nA Linux (Ubuntu 22.04) computer with 8 Go RAM and a 4 cores processor: The code failed to run due to a lack of memory.\nA Linux pod (Ubuntu 22.04) on a Kubernetes server (Onyxia/SSP Cloud) running aDocker image with R and R Studio on which we install a python distribution (miniconda) with the R package {reticulate} and a 18.5 Julia distribution with the R package {JuliaCall}. The script files 002 - dl_IUCN.R, 003 - species_richness.py and 004 - join_rasters.R run successfully (after several trials and errors and including some corrections in the code, documented in the following chapters). However, the script 005 - covariate_matching.jl fails to start running, apparently because Julia does not successfully identify dependencies required by the ArchGDAL package (CURL_4), dependencies that are nevertheless present on the system.\nA Linux pod (Ubuntu 22.04) on a Kubernetes server (Onyxia/SSP Cloud) running aDocker image with python and Julia where we install R and spatial dependenties with Linux apt package manager (following a procedure documented by ThinkR on RTasks). The scripts files 002 - dl_IUCN.R, 003 - species_richness.py and 004 - join_rasters.R run succesfully and the script 005 - covariate_matching.jl starts running, but fails further down in the execution process. Because it happens in parallel processing, the error messages are not meaningful and we need to contact the authors to help us identify the cause of the error.\n\nWorking with Kubernetes pods enables to mobilize large memory and processing resources, and to flexibly adapt the configuration. However, pods must be deleted after use and re-created for each new use, which is time consuming. With the help of Onyxia admin, we are preparing a docker image that includes R, python and Julia with all the required package. This should make quicker the re-creation process."
  },
  {
    "objectID": "replication_report/03_computing_setup.html#technical-environment-and-prerequisites",
    "href": "replication_report/03_computing_setup.html#technical-environment-and-prerequisites",
    "title": "3  Computing environment setup",
    "section": "3.2 Technical environment and prerequisites",
    "text": "3.2 Technical environment and prerequisites\nWe document the execution process by including all package installation, code modifications and script execution in a literate programming format. It is possible to combine different programming languages in litterate programming platforms such as Jupyter or RMarkdown, or its new generation Quarto. We decided to use Quarto because of its versatility and our familiarity of this tool. Quarto can be obtained at www.quarto.org.\nThe code also requires Linux or a Windows machine running Windows subsystem for Linux, otherwise a substantial rewriting of some scripts is needed. We run it on a Windows personal computer. We have not tried, but it might be possible to run it on Mac as it is similar to Linux in several regards (both are unix systems).\nThe first script 001 - dl_GEE.py requires to have a gmail account to access google services and to registrer on Google Earth Engine."
  },
  {
    "objectID": "replication_report/03_computing_setup.html#sec-rsetup",
    "href": "replication_report/03_computing_setup.html#sec-rsetup",
    "title": "3  Computing environment setup",
    "section": "3.3 Set up R",
    "text": "3.3 Set up R\nThe following script installs the R dependies called in the code. We are not certain that all dependencies are effectively used in the script.\n\n\nCode\n# Install from Github --------------------------------------------------------\n\n# Some packages need to be installed from developper sources because there are\n# not available or official sources have some issues.\n# Installing version 1.3.5 which is the last working version apparently\n\nif (system.file(package = \"doMC\") == \"\") {\n  remotes::install_github(\"https://github.com/cran/doMC/tree/fbea362b96cc4469deb6065ff9fbd5d4794ccac1\")\n} \nif (system.file(package = \"gdalUtils\") == \"\") {\n  remotes::install_github(\"https://github.com/cran/gdalUtils\", upgrade = FALSE)\n} \nif (system.file(package = \"ggeasy\") == \"\") {\n  remotes::install_github(\"jonocarroll/ggeasy\")\n} \nif (system.file(package = \"velox\") == \"\") {\n  remotes::install_github(\"https://github.com/hunzikp/velox\", upgrade = FALSE)\n} \nif (system.file(package = \"rnaturalearth\") == \"\") {\n  remotes::install_github(\"https://github.com/ropensci/rnaturalearth\")\n} \nif (system.file(package = \"gdalUtils\") == \"\") {\n  remotes::install_github(\"gearslaboratory/gdalUtils\")\n} \nif (system.file(package = \"geoarrow\") == \"\") {\n  remotes::install_github(\"paleolimbot/geoarrow\")\n} \n\n\n# Install from CRAN ------------------------------------------------------------\n\n# These packages are available from the usual source from R\nrequired_packages <- c( # List all required R packages\n  \"reticulate\", # To interact with python (normally installed with Quarto)\n  \"JuliaCall\", # To interact with Julia \n  \"tidyverse\", # To facilitate data manipulation\n  \"aws.s3\", # to interact with S3\n  # All packages below are used in Wolf and al. code files:\n  \"countrycode\",\n  \"cowplot\",\n  \"data.table\",\n  \"dtplyr\",\n  \"fasterize\",\n  \"foreach\",\n  \"foreign\",\n  \"ggforce\",\n  \"ggplot2\",\n  \"ggrepel\",\n  \"GpGp\",\n  \"grid\",\n  \"jsonlite\",\n  \"landscapetools\",\n  \"lme4\",\n  \"MCMC.OTU\",\n  \"ncdf4\",\n  \"parallel\",\n  \"pbapply\",\n  \"plyr\",\n  \"raster\",\n  \"rasterVis\",\n  \"rbounds\",\n  \"RColorBrewer\",\n  \"RCurl\",\n  \"readr\",\n  \"reshape2\",\n  \"rgdal\",\n  \"rjson\",\n  \"rnaturalearth\",\n  \"scales\",\n  \"sf\",\n  \"smoothr\",\n  \"spaMM\",\n  \"spgwr\",\n  \"spmoran\",\n  \"spNNGP\",\n  \"stars\",\n  \"stringr\",\n  \"tidyverse\",\n  # \"unix\",\n  \"velox\",\n  \"viridis\",\n  \"wbstats\",\n  \"wdpar\") \nmissing <- !(required_packages %in% installed.packages())\n\n# Install \nif(any(missing)) install.packages(required_packages[missing], \n                                  repos = \"https://cran.irsn.fr/\")"
  },
  {
    "objectID": "replication_report/03_computing_setup.html#set-up-python",
    "href": "replication_report/03_computing_setup.html#set-up-python",
    "title": "3  Computing environment setup",
    "section": "3.4 Set up python",
    "text": "3.4 Set up python\nThe following scripts install a python distribution (miniconda) with the R package reticulate. It also installs several python packages. Note that several of these packages are not effectively called within the R code.\n\n\nCode\nlibrary(reticulate) # to run python from R\n\n# Variables to modify\nmy_envname <- \"replication-wolf\"\nscripts_to_run <- c(\"003\") # or c(\"001\", \"003\") or \"001\"\n\n# Install python if not already present\nif (!dir.exists(miniconda_path())) {\n  install_miniconda()\n}\n# conda_remove(my_envname) # for debugging purposes\n# Create environment if not already\nif (!my_envname %in% conda_list()$name) {\n  conda_create(my_envname)\n}\n# Packages needed for each script\nrequirements <- list(\n  \"001\" = c(\"earthengine-api\", \"rasterio\", \"pandas\", \"pydrive\"),\n  \"003\" = c(\"fiona\", \"rasterio\", \"ray[default]\", \"dbfread\", \"pandas\"))\n# Combined depending on the variable defined at beginning of code chunk\nrequired <- unique(unlist(requirements[scripts_to_run]))\n# identify which ones are missing\npy_installed_packages <- py_list_packages(my_envname)$package\nmissing_packages <- required[!required %in% py_installed_packages]\n# Install those\nif (length(missing_packages) > 0) {\n  conda_install(envname = my_envname, \n              packages = missing_packages,\n              pip = TRUE)\n}\n\n# Adding a dependency required on linux platforms\n if ((Sys.info()[\"sysname\"] == \"Linux\") &\n (!\"libstdcxx-ng\" %in% py_installed_packages)) {\n  conda_install(envname = my_envname,\n              packages = \"libstdcxx-ng\")\n  } \n    \n\n# Activate the corresponding environment\nuse_condaenv(my_envname)\n\n\n\n\nCode\nlibrary(reticulate) # to run python from R\n\n# Variables to modify\nmy_envname <- \"replication-wolf2\"\n\n# py_list_packages(my_envname)\n# system(\"strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXX\")\n# system('conda install -c \"conda-forge/label/gcc7\" libstdcxx-ng')\n# \n# conda_install()\n\n# Install python if not already present\nif (!dir.exists(miniconda_path())) {\n  install_miniconda()\n}\n\n# Create environment if not already\nif (my_envname %in% conda_list()$name) {\n  conda_remove(my_envname)\n} \nconda_create(my_envname)\n\nrequirements_pip <- c(\"fiona\", \"ray[default]\", \"rasterio\", \"ray\", \"dbfread\", \"pandas\")\nrequirements_conda <- c(\"libstdcxx-ng\")\n\n# identify which ones are missing\npy_installed_packages <- py_list_packages(my_envname)$package\nmissing_pip <- requirements_pip[!requirements_pip %in% py_installed_packages]\nmissing_conda <- requirements_conda[!requirements_conda %in%\n                                      py_installed_packages]\n\n# Install those\nif (length(missing_pip) > 0) {\n  conda_install(envname = my_envname, \n              packages = missing_pip,\n              pip = TRUE)\n}\nif (length(missing_conda) > 0) {\n  conda_install(envname = my_envname, \n              packages = missing_conda)\n}\n\n# Activate the corresponding environment\nuse_condaenv(my_envname)\n\n\nCreate an authorization\n\n\nCode\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\n\ngauth = GoogleAuth()\n# Try to load saved client credentials\ngauth.LoadCredentialsFile(\"mycreds.txt\")\nif gauth.credentials is None:\n    # Authenticate if they're not there\n    gauth.LocalWebserverAuth()\nelif gauth.access_token_expired:\n    # Refresh them if expired\n    gauth.Refresh()\nelse:\n    # Initialize the saved creds\n    gauth.Authorize()\n# Save the current credentials to a file\ngauth.SaveCredentialsFile(\"mycreds.txt\")"
  },
  {
    "objectID": "replication_report/03_computing_setup.html#set-up-julia",
    "href": "replication_report/03_computing_setup.html#set-up-julia",
    "title": "3  Computing environment setup",
    "section": "3.5 Set up Julia",
    "text": "3.5 Set up Julia\nFirst we install or set-up Julia from R if needed.\n\n\nCode\nlibrary(JuliaCall)\n\nif (!dir.exists(paste0(rappdirs::user_data_dir(), \"/R/JuliaCall/julia\"))) {\n  install_julia()\n}\nif (!exists(\"my_julia\")) {\n  my_julia <- julia_setup()\n}\njulia_install_package_if_needed(\"ArchGDAL\")\njulia_install_package_if_needed(\"DataFrames\")\njulia_install_package_if_needed(\"Discretizers\")\njulia_install_package_if_needed(\"Shapefile\")\njulia_install_package_if_needed(\"FreqTables\")\njulia_install_package_if_needed(\"Plots\")\njulia_install_package_if_needed(\"StatsBase\")\njulia_install_package_if_needed(\"CSV\")\njulia_install_package_if_needed(\"LibGEOS\")"
  },
  {
    "objectID": "replication_report/03_computing_setup.html#adapt-to-windows-system-optionnal",
    "href": "replication_report/03_computing_setup.html#adapt-to-windows-system-optionnal",
    "title": "3  Computing environment setup",
    "section": "3.6 Adapt to Windows system (optionnal)",
    "text": "3.6 Adapt to Windows system (optionnal)\nThe script 002 - dl_IUCN.R includes a system command that wget that refers to a fownloading software that is included in UNIX platforms (Mac and Linux).\nOn Linux, if wget is not available, the user must run the same commands without wsl, that is run the command sudo apt-get update followed by the command sudo apt-get install parallel.\n\n\nCode\nif (Sys.info()[\"sysname\"] == \"Linux\") {\n  system(\"sudo apt update\")\n  system(\"sudo apt install -y parallel\")\n} \n\n\nIt is possible however to run it on Windows, if and only the Windows system includs Windows Subsystem for Linux. In that case, if wsl is not already installed the user must first install parallel that includes wget, runing the command wsl sudo apt-get update followed by the command wsl sudo apt-get install parallel.\n\n\nCode\n# Replace all wget calls by wsl wget\nif (Sys.info()[\"sysname\"] == \"Windows\") {\n  replace_all(\"002 - dl_IUCN.R\", \"wget\", \"wsl wget\")\n}"
  },
  {
    "objectID": "replication_report/04_data_preparation.html#code-updates-to-fix-automatic-downloads",
    "href": "replication_report/04_data_preparation.html#code-updates-to-fix-automatic-downloads",
    "title": "4  Data gathering and preparation",
    "section": "4.1 Code updates to fix automatic downloads",
    "text": "4.1 Code updates to fix automatic downloads\nSeveral data sources are automatically fetched when running the source code, but several links and APIs are broken and need updating:\n\nThe country borders are fetched using the package {rnaturalearth} but the source API has evolved and the official version of the package is broken. A development version must be installed from Github (see Section 3.3) ;\nThe URL for Curtis et al. (2018) data from the Science portal is broken and needs to be replaced.\nThe URL for the WWF biome areas also needs to be updated.\n\nThe following code performs these modifications.\n\n\nCode\nlibrary(tidyverse) # A library bundle to ease data management\n\n# Copy the code from Github ---------------------------------------------------\nsetwd(\"/home/onyxia/work/PA_matching/replication_report\")\n# Not a good practice, only for debugging\n# setwd(\"replication_report\")\n# URL pointing to the zip download of the Wolf et al. repository\nrepo <- \"https://codeload.github.com/wolfch2/PA_matching/zip/refs/heads/master\"\n\n# downloads locally\ndownload.file(repo, destfile = \"original.zip\", method = \"curl\")\n# unzips to a containing folder named PA_matching-master (github default)\nunzip(\"original.zip\")\n# original folder will remain intact, we make a copy with required edits to run\ndir.create(\"PA_matching_asis\")\nfile.copy(list.files(\"PA_matching-master\", full.names = TRUE), \n          \"PA_matching_asis\", overwrite = TRUE)\nfile.remove(\"original.zip\")\n\n# A function to replace some parts of the code ---------------------------------\nreplace_all <- function(pattern, replacement, file) {\n  readLines(file) %>%\n    str_replace_all(pattern, replacement) %>%\n    writeLines(file)\n}\n\n# Change the source paths ------------------------------------------------------\njoin_script <- \"PA_matching_asis/004 - join_rasters.R\"\n# Replace Science URL that has changed\nold <- \"https://science.sciencemag.org/highwire/filestream/715492/field_highwire_adjunct_files/2/aau3445-Data-S3.tif\"\nnew <- \"https://www.science.org/action/downloadSupplement?doi=10.1126%2Fscience.aau3445&file=aau3445-data-s3.tif\"\nreplace_all(pattern = old,\n            replacement = new,\n            file = join_script)\n# Replace Curtis et al filename: lowercased by Science portal admins\nreplace_all(pattern = \"data_input/aau3445-Data-S3.tif\",\n            replacement = \"data_input/aau3445-data-s3.tif\",\n            file = join_script)\n# Replace url to download WWF data\nold <- \"https://c402277.ssl.cf1.rackcdn.com/publications/15/files/original/official_teow.zip\\\\?1349272619\"\nnew <- \"https://files.worldwildlife.org/wwfcmsprod/files/Publication/file/6kcchn7e3u_official_teow.zip\"\nreplace_all(pattern = old,\n            replacement = new,\n            file = join_script)\nreplace_all(pattern = \"data_input/official_teow.zip\\\\?1349272619\",\n            replacement = \"data_input/6kcchn7e3u_official_teow.zip\",\n            file = join_script)"
  },
  {
    "objectID": "replication_report/04_data_preparation.html#data-that-needs-to-be-fetched-manually-before-running-code",
    "href": "replication_report/04_data_preparation.html#data-that-needs-to-be-fetched-manually-before-running-code",
    "title": "4  Data gathering and preparation",
    "section": "4.2 Data that needs to be fetched manually before running code",
    "text": "4.2 Data that needs to be fetched manually before running code\nSeveral sources needs to be collected manually and stored in the file tree to run the code. After manual download stored these data in an encrypted archives on a private S3 storage. We also store the file encryption in a Vault instance.\n\n\nCode\nlibrary(tidyverse)\nlibrary(aws.s3)\n\n# Gather secrets required to work with S3 systems\nif (Sys.info()[\"sysname\"] == \"Windows\") {\n  if (file.exists(\"secrets_aws.R\")) {\n    source(\"secrets_aws.R\")\n  } else { \n    print(paste0(\n    \"You must save your AWS credentials in a local file named secret_aws.R. \",\n    \"For more indications see: https://www.book.utilitr.org/\",\n    \"sspcloud.html#renouveler-des-droits-dacc%C3%A8s-p%C3%A9rim%C3%A9s\"))\n  }\n}\n\n# If running on SSP Cloud, getting the key from vault\nif (Sys.info()[\"effective_user\"] == \"onyxia\") {\n  system(\"vault kv get -format=table onyxia-kv/fbedecarrats/species_info\", \n         intern = TRUE) -> my_secret \n  my_key_zip <- my_secret %>%\n    pluck(18) %>%\n    str_extract(\"[:alnum:]*$\")\n  my_key_api <- my_secret %>%\n    pluck(17) %>%\n    str_extract(\"[:alnum:]*$\")\n} else {\n my_secret <- readLines(\"secret_zip_key\")\n}\n\n# A function to put data from local machine to S3\nput_to_s3 <- function(from, to) {\n  aws.s3::put_object(\n    file = from,\n    object = to,\n    bucket = \"fbedecarrats\",\n    region = \"\",\n    multipart = TRUE)\n}\n\n# A function to iterate/vectorize copy\nsave_from_s3 <- function(from, to) {\n  aws.s3::save_object(\n    object = from,\n    bucket = \"fbedecarrats\",\n    file = to,\n    overwrite = FALSE,\n    region = \"\")\n}\n\n# Listing files in bucket\nmy_files <- get_bucket_df(bucket = \"fbedecarrats\",\n                          prefix = \"Replication_wolf\",\n                          region = \"\")\n\n\n\n4.2.1 Species data\nThe published article mentions that the sources are publicly available, but several sources are not in open data, that means that they are not directly downloadable. This refers to three sources :\n\nthe species range maps from IUCN (include mamals, amphibians and reptiles) must be fetched from the IUCN Spatial data download webpage. They must be unzipped and placed in the folder data_input/range_maps ;\nFor the bird species range map from Birds of the world, a query must be submitted via email to BirdLife with the elements described on the dedicated webpage. The authorization takes about two weeks to be assessed. If/after authorization, the administrators send the user a link to download the data.\nIt is also required to generate a token from the UICN website and include it in the line 18 of the script 002 - dl_IUCN.R for it to run successfully.\n\n\n\nCode\n# Fetch and unzip species range maps -------------------------------------------\n\n# We stored the range maps in encrypted zip (sensitive data)\n# Secrets are stored in a Vault accessed above\nsave_from_s3(from = \"Replication_wolf/data_input/range_maps/iucn_species_classes.zip\",\n             to = \"iucn_species_classes.zip\")\nsystem(paste(\"unzip -P\", my_key_zip, \"iucn_species_classes.zip\"))\nfile.remove(\"iucn_species_classes.zip\")\n\nsave_from_s3(from = \"Replication_wolf/data_input/range_maps/BOTW.zip\",\n             to = \"BOTW.zip\")\nsystem(paste(\"unzip -P\", my_key_zip, \"BOTW.zip\", \" -d data_input/range_maps/\"))\nfile.remove(\"BOTW.zip\")\n\n# Merge reptile files: uncomment to run ----------------------------------------\n\n# # Note that the Reptile species range map comes in 2 parts that need to be\n# # merged. The following code performs this operation\n# \n# reptiles1 <- st_read(\"data_input/range_maps/REPTILES_PART1.shp\")\n# \n# reptiles1 %>%\n#   select(-OBJECTID) %>%\n#   st_write(dsn = \"data_input/range_maps/REPTILES_PART2.shp\", append = TRUE)\n# \n# file.rename(list.files(path = \"data_input/range_maps\", \n#                        pattern =\"REPTILES_PART2\", full.names = TRUE),\n#             str_replace(list.files(path = \"data_input/range_maps\",\n#                                    pattern=\"REPTILES_PART2\", \n#                                    full.names = TRUE),\n#                         pattern=\"REPTILES_PART2\", \"REPTILES\"))\n\n\n\n\n4.2.2 Data on protected areas\nProtected planet only provides the latest version of the WDPA. We could not figure out how to obtain the versions from previous months or years. Wolf et al. used the January 2020 version, we use the January 2023. The dataset includes the year of creation of the status, which enables to apply the same temporal filters than the authors (Protected areas created between from 2001 and 2019), but it is possible that information about existing entities have been added, modified or deleted between January 2020 and January 2021.\nWe encountered several obstacles for this operation, that we document below.\nWe initially tried to obtain it through the package {wdpar}, with the following code. However, when using this method, a column named GIS_AREA is not present in the data, while it is used by the script by Wolf et al. performing the matching (005 - covariate_matching.jl).\n\n\nCode\nlibrary(aws.s3)\nlibrary(tidyverse)\nlibrary(wdpar)\nlibrary(sf)\n\n# Found the previous version of WDPA\nwdpa_Jan2020_url <- \"https://pp-import-production.s3-eu-west-1.amazonaws.com/WDPA_Jan2020_Public.zip\"\n\nsave_from_s3(from = \"Replication_wolf/WDPA_Jan2023_Public.gdb.zip\",\n             to = \"data_input/WDPA_Jan2023_Public.gdb.zip\")\nsave_from_s3(from = \"Replication_wolf/data_input/PAs/WDPA_Jan2023_poly.parquet\",\n             to = \"data_input/PAs/WDPA_Jan2023_poly.parquet\")\n\n# Get the data from protectedplanet\nif (file.exists(\"data_input/WDPA_Jan2023_Public.gdb.zip\")) {\n  wdpa_global <- wdpa_read(\"data_input/WDPA_Jan2023_Public.gdb.zip\")\n} else {\n  wdpa_global <- wdpa_fetch(\"global\", download_dir = \"data_input\")\n}\n\n# Build a name with same structure than Wolf et al.\nwdpa_file_name <- list.files(\"data_input\", pattern = \".gdb.zip\") %>%\n  str_remove(\"_Public.gdb.zip\") %>%\n  paste0(\"data_input/PAs/\", ., \"-shapefile-polygons.shp\")\n# Filter for polygons (filtering out points to be sure not to exclude something\n# by mistake) and write locally.\nwdpa_global %>%\n  #\"We excluded PAs from our primary analysis that: \n  #[1] had point (centroid) information only, \n  filter(st_geometry_type(.) != \"MULTIPOINT\") %>%\n  #[2] were exclusively marine, \n  filter(MARINE != \"2\") %>% #0: 100% terrestrial, 1: both, 2: 100% marine\n  #[3] were established after 2000 since the forest loss data range from 2001 to 2018,\n  filter(STATUS_YR > 2000) %>%\n  #[4] had area less than 1 km2 since forest change in small PAs can be hard to\n  # estimate accurately, \n  filter(REP_AREA - REP_M_AREA >= 1) %>%\n  st_write(dsn = wdpa_file_name)\n\n# 48066 PAs filtered. Check as this seems quite low compared to 286356 in the \n# WDPA global database.\n\n\nWe then downloaded the data as zipped shapefiles from protectedplanet.net and placed it in the folder replication_report/temp. We executed the following script to store it in the same format than referred to in Wolf et al. source code. However, the WDPA polygons include some topological errors and the code failed. We finally chose to store the data in geoparquet format, which is more performant and more lenient regarding topological validity.\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(aws.s3)\nunzip(zipfile = \"temp/WDPA_Jan2023_Public_shp.zip\", \n      exdir = \"temp\")\nunzip(zipfile = \"temp/WDPA_Jan2023_Public_shp_0.zip\",\n      exdir = \"temp/shp0\")\nunzip(zipfile = \"temp/WDPA_Jan2023_Public_shp_1.zip\",\n      exdir = \"temp/shp1\")\nunzip(zipfile = \"temp/WDPA_Jan2023_Public_shp_2.zip\",\n      exdir = \"temp/shp2\")\nwdpa <- st_read(\"temp/shp0/WDPA_Jan2023_Public_shp-polygons.shp\") %>%\n  bind_rows(st_read(\"temp/shp1/WDPA_Jan2023_Public_shp-polygons.shp\")) %>%\n  bind_rows(st_read(\"temp/shp2/WDPA_Jan2023_Public_shp-polygons.shp\"))\nst_write(obj = wdpa, \n         dsn = \"data_input/PAs/WDPA_Jan2023_Public_shp-polygons.shp\")\n\nwdpa2 <- wdpa %>%\n  #\"We excluded PAs from our primary analysis that: \n  #[1] had point (centroid) information only, \n  # filter(st_geometry_type(.) != \"MULTIPOINT\") %>% already OK in shapefile\n  #[2] were exclusively marine, \n  filter(MARINE != \"2\") %>% #0: 100% terrestrial, 1: both, 2: 100% marine\n  #[3] were established after 2000 since the forest loss data range from 2001 to 2018,\n  filter(STATUS_YR > 2000) %>%\n  #[4] had area less than 1 km2 since forest change in small PAs can be hard to\n  # estimate accurately, \n  filter(GIS_AREA >= 1)\n\ndir.create(\"data_input/PAs/v2\")\nst_write(obj = wdpa2, \n         dsn = \"data_input/PAs/v2/WDPA_Jan2023_Public_shp-polygons.shp\")\n\ndir.create(\"data_input/PAs/v3\")\nremotes::install_github(\"paleolimbot/geoarrow\")\nlibrary(geoarrow)\nwrite_geoparquet(wdpa2, \"data_input/PAs/v3/WDPA_Jan2023_poly.parquet\")\n\nput_to_s3(from = \"data_input/PAs/v3/WDPA_Jan2023_poly.parquet\",\n          to = \"Replication_wolf/data_input/PAs/WDPA_Jan2023_poly.parquet\")\n\n## TO ADD : script modification new/old\n# PAs <- geoparquet::read_reoparquet_sf(\"data_input/PAs/WDPA_Jan2023_poly.parquet\")\n# PAs = read_sf(\"data_input/PAs/WDPA_Jan2023-shapefile-polygons.shp\")"
  },
  {
    "objectID": "replication_report/04_data_preparation.html#fetch-source-data",
    "href": "replication_report/04_data_preparation.html#fetch-source-data",
    "title": "4  Data gathering and preparation",
    "section": "4.3 Fetch source data",
    "text": "4.3 Fetch source data\nget the right WDPA data. Wolf et al. use the data from January 2020."
  },
  {
    "objectID": "replication_report/04_data_preparation.html#prepare-data",
    "href": "replication_report/04_data_preparation.html#prepare-data",
    "title": "4  Data gathering and preparation",
    "section": "4.4 Prepare data",
    "text": "4.4 Prepare data\nThe data from IUCN now comes in 2 parts for the reptiles.\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)"
  },
  {
    "objectID": "replication_report/04_data_preparation.html#get-pre-processed-milestones",
    "href": "replication_report/04_data_preparation.html#get-pre-processed-milestones",
    "title": "4  Data gathering and preparation",
    "section": "4.5 Get pre-processed milestones",
    "text": "4.5 Get pre-processed milestones\n\n\nCode\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(aws.s3)\nlibrary(purrr)\nlibrary(stringr)\n\nif (Sys.info()[\"sysname\"] == \"Windows\") {\n  source(\"secrets_aws.R\")\n}\n\n# Listing files in bucket\nmy_files <- get_bucket_df(bucket = \"fbedecarrats\",\n                          prefix = \"Replication_wolf\",\n                          region = \"\")\n# Selecting tifs at root\nmy_tifs <- my_files %>%\n  filter(str_ends(Key, \".tif\")) %>%\n  pluck(\"Key\")\n\n# Generating the destination paths\nmy_tifs_dest <- my_tifs %>%\n  str_replace(\"data_processed/elev.tif\", \"data_processed/rasters/elev.tif\") %>%\n  str_replace(\"Replication_wolf/data_processed\", \"data_processed\") %>%\n  str_replace(\"Replication_wolf\", \"data_input\")\n\n# transfer_tif <- tibble(my_tifs, my_tifs_dest)\n\n# A function to iterate/vectorize copy\nsave_from_s3 <- function(x, y) {\n  aws.s3::save_object(\n    object = x,\n    bucket = \"fbedecarrats\",\n    file = y,\n    overwrite = FALSE,\n    region = \"\")\n  }\n# copy from S3 to local\nmap2(my_tifs, my_tifs_dest, save_from_s3)\n\n# Select PA shp\nmy_PAs <- my_files %>%\n  filter(str_detect(Key, \"/PAs/WDPA\")) %>%\n  pluck(\"Key\")\n# Create paths in local machine\nmy_PAs_dest <- my_PAs %>%\n  str_replace(\"Replication_wolf/\", \"data_input/\")\n# Retrieve the data\nmap2(my_PAs, my_PAs_dest, save_from_s3)\n\n# # Select WWF shp\n# my_biomes <- my_files %>%\n#   filter(str_detect(Key, \"/official/\")) %>%\n#   pluck(\"Key\")\n# # Create paths in local machine\n# my_biomes_dest <- my_biomes %>%\n#   str_replace(\"Replication_wolf/\", \"data_input/\")\n# # Retrieve the data\n# map2(my_biomes, my_biomes_dest, save_from_s3)\n\n# # Create a function to put data from local machine to S3\n# put_to_s3 <- function(x, y) {\n#   aws.s3::put_object(\n#     file = x,\n#     object = y,\n#     bucket = \"fbedecarrats\",\n#     region = \"\",\n#     multipart = TRUE)\n# }\n# \n# # Apply it to the PAs\n# wdpa_local_files <- list.files(\"data_input/PAs\", full.names = TRUE)\n# wdpa_s3_objects <- wdpa_local_files %>%\n#   str_replace(\"data_input\", \"Replication_wolf\")\n# map2(wdpa_local_files, wdpa_s3_objects, put_to_s3)\n# # Also to the source\n# put_to_s3(\"data_input/WDPA_Jan2023_Public.gdb.zip\",\n#           \"Replication_wolf/WDPA_Jan2023_Public.gdb.zip\")\n# # Single put\n# aws.s3::put_object(\n#   file = \"data_input/aau3445-Data-S3.tif\",\n#   object = \"Replication_wolf/aau3445-Data-S3.tif\",\n#   bucket = \"fbedecarrats\",\n#   region = \"\",\n#   multipart = TRUE)\n\n\n\n\n\n\nCurtis, Philip G., Christy M. Slay, Nancy L. Harris, Alexandra Tyukavina, and Matthew C. Hansen. 2018. “Classifying Drivers of Global Forest Loss.” Science 361 (6407): 11081111."
  },
  {
    "objectID": "replication_report/05_reproduction_as_is.html#original-source-code-execution",
    "href": "replication_report/05_reproduction_as_is.html#original-source-code-execution",
    "title": "5  Reproduction “as is” of the publised results",
    "section": "5.1 Original source code execution",
    "text": "5.1 Original source code execution\nSome code modifications are required for the code to run. These edits are not intended to test the robustness of the analysis, just for the successful run of the scripts.\nWe will use the following function that explicitly declares which modification are made in the source code.\n\n\nCode\nlibrary(tidyverse)\n\nif (!stringr::str_detect(getwd(), \"PA_matching\")) {\n  setwd(\"PA_matching\")\n}\nif (!stringr::str_ends(getwd(), \"replication_report\")) {\n  setwd(\"replication_report\")\n}\n# A function to replace some parts of the code\nreplace_all &lt;- function(pattern, replacement, file) {\n  readLines(file) %&gt;%\n    str_replace_all(pattern, replacement) %&gt;%\n    writeLines(file)\n}\n\n\n\n5.1.1 Fetch data from Google Earth Engine\nMofify the paths\n\n\nCode\nGEE_data_script &lt;- \"PA_matching_asis/001 - dl_GEE.py\"\nwolf_path &lt;- \"/home/chrisgraywolf/shared/analysis/PA_matching/data_input/\"\nmy_data_input &lt;- paste0(getwd(), \"/data_input/\")\nreplace_all(pattern = wolf_path ,\n            replacement = my_data_input,\n            file = GEE_data_script)\n\n\nDo-not re-execute this code: fetch existing data instead.\n\n\nCode\nGEE_data_script &lt;- \"PA_matching_asis/001 - dl_GEE.py\"\nsource_python(GEE_data_script)\n\n\n\n\nCode\n# The code below saves the output data to a private data lake.\ngee_pattern &lt;- \"(land|elev|loss|gain|cover|mask|travel_time|pop_dens).*tif\"\n# List the outputs of the GEE data fetching\ngee_local &lt;- list.files(path = \"data_input\", full.names = TRUE,\n                  pattern = paste0(\"^\", gee_pattern))\n# Transform to specify their location on S3\ngee_to_s3 &lt;- paste0(\"Replication_wolf/\", gee_local)\n# Send to S3\nmap2(gee_local, gee_to_s3 , put_to_s3)\n\n\n\n\nCode\n# fetch from S3 the data gathered from Google Earth Engine\n\n# The code below fetches the output data from a private data lake.\n# naming pattern of files fetched from Google Earth Engine\ngee_pattern &lt;- \"(land|elev|loss|gain|cover|mask|travel_time|pop_dens).*tif\"\n# Filter those files from bucket content\ngee_in_s3 &lt;- my_files %&gt;%\n  filter(str_detect(Key, \n                    paste0(\"Replication_wolf/data_input.\", gee_pattern))) %&gt;%\n  pluck(\"Key\")\n# rename for local location\ngee_to_local &lt;- str_remove(gee_in_s3, \"Replication_wolf/\")\n# Copy locally\nmap2(gee_in_s3, gee_to_local, save_from_s3)\n\n\n\n\n5.1.2 Fetch species data from IUCN\nModify the script to enable it to run on another machine.\nThe column names in the official BOTW database are now in lowercase. It was apparently in uppercase when Wolf et al. processed the data. We need to modify the reference to one column name in the script for it to run.\n\n\nCode\nIUCN_data_script &lt;- \"PA_matching_asis/002 - dl_IUCN.R\"\nmy_wd &lt;- paste0(getwd(), \"/\")\nwolf_path &lt;- \"/home/chrisgraywolf/analysis_desktop/PA_matching/\"\n\nreplace_all(pattern = wolf_path, replacement = my_wd, file = IUCN_data_script)\nreplace_all(pattern = \"put_your_token_here\", replacement = my_key_api,\n            file = IUCN_data_script)\n# Column name to lowercase\nreplace_all(pattern = \"SISID\", replacement = \"sisid\", file = IUCN_data_script)\n\n\nAdd the required input data\nRun the script\n\n\nCode\nsource(IUCN_data_script)\n\n\n\n\nCode\n# Save the output data to S3\n\n# Species data csv ------------------------------------------------------------\nput_to_s3(from = \"data_processed/species_data.csv\", \n          to = \"Replication_wolf/data_processed/species_data.csv\")\n\n# Bird shapefiles -------------------------------------------------------------\n# List the bird shapefiles created by 002 - dl_IUCN.R\nbird_shps &lt;- list.files(path = \"data_input/range_maps\",\n                        pattern = \"BIRDS_.*\", full.names = TRUE)\n# Bundle them in an encrypted zip locally\nsystem(paste0(\"zip -P \", my_key_zip, \" birds_shps.zip \",\n       paste(bird_shps, collapse = \" \")))\n# Send the local zip to S3\nput_to_s3(from = \"birds_shps.zip\", \n          to = \"Replication_wolf/data_input/range_maps/birds_shps.zip\")\n# Delete the local zip\nfile.remove(\"birds_shps.zip\")\n\n# elev.tif --------------------------------------------------------------------\n# The main output of the script.\nput_to_s3(from = \"data_processed/rasters/elev.tif\", \n          to = \"Replication_wolf/data_processed/rasters/elev.tif\")\n\n\n\n\nCode\n# Load the data \nsave_from_s3(from = \"Replication_wolf/data_processed/species_data.csv\",\n             to = \"data_processed/species_data.csv\")\n\n# Fetch birds from S3\nsave_from_s3(from = \"Replication_wolf/data_input/range_maps/birds_shps.zip\",\n             to = \"birds_shps.zip\")\nsystem(paste(\"unzip -P\", my_key_zip, \"birds_shps.zip\"))\nfile.remove(\"birds_shps.zip\")\n\n# species_data &lt;- read_csv(\"data_processed/species_data.csv\")\nsave_from_s3(from = \"Replication_wolf/data_processed/rasters/elev.tif\",\n             to = \"data_processed/rasters/elev.tif\")\n\n\n\n\n5.1.3 Compute species richness\nWe need to replace the specific path used by Wolf et al with a more generic path. 18 python packages and package modules are imported by the script but are not used in the analysis. We comment them to avoid dependency complications. The ray package is designed to work with some virtual memory that is not present on our configuration. We need to add a mention an option to enable ray to work with physical memory. We also need to modify the following command richness = results[0], as it correspond to a method to assign an array that is not allowed in recent numpy versions and returns an error. We therefore add .copy() to this command, to have richness = results[0].copy(), which now correspond to the correct way of proceeding with this operation in python.\n\n\nCode\n# Set target\nspecies_richness_script &lt;- \"PA_matching_asis/003 - species_richness.py\"\n# change path\nreplace_all(pattern = \"/home/chrisgraywolf/analysis_desktop/PA_matching/\",\n            replacement = paste0(getwd(), \"/\"),\n            file = species_richness_script)\n\n# List unused packages and modules\nunused_packages &lt;- c(\"affine\", \"ctypes\", \"functools\", \"geojson\", \"geopandas\", \n                     \"glob\", \"math\", \"matplotlib\", \"multiprocessing\", \n                     \"operator\", \"psutil\", \"random\", \"shapely\")\nunused_package_modules &lt;- c(\"collections\", \"fiona\", \"osgeo\", \"pyproj\")\n\n# Comment the corresponding lines\nmap(unused_packages, ~replace_all(pattern = paste0(\"^import \", .x, \".*$\"), \n                                  replacement = \"\\\\# \\\\0\",\n                                  file = species_richness_script))\nmap(unused_package_modules, ~replace_all(pattern = paste0(\"^from \", .x, \".*$\"), \n                                         replacement = \"\\\\# \\\\0\",\n                                         file = species_richness_script))\n# Add option to enable ray to use physical memory\nray_to_add &lt;- paste(\"import os\",\n                    \"os.environ[\\\"RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE\\\"] = \\\"1\\\"\",\n                    \"import ray\",\n                    sep = \"\\n\")\nreplace_all(pattern = \"import ray\",\n            replacement = ray_to_add,\n            file = species_richness_script)\n# Ray current modules\nreplace_all(pattern = \"pool = ray.experimental.ActorPool(actors)\",\n            replacement = \"pool = ray.util.ActorPool(actors)\",\n            file = species_richness_script)\n# Correct array assignment\nreplace_all(pattern = \"richness = results[0]$\",\n            replacement = \"richness = results[0].copy()$\",\n            file = species_richness_script)\n\n\nRun the file\n\n\nCode\nlibrary(reticulate)\nmy_envname &lt;- \"replication-wolf\"\nuse_condaenv(my_envname)\nspecies_richness_script &lt;- \"PA_matching_asis/003 - species_richness.py\"\nsource_python(species_richness_script)\n\n\n\n\nCode\n# Save outputs\nput_to_s3(from = \"data_processed/non-threatened.tif\", \n          to = \"Replication_wolf/data_processed/non-threatened.tif\")\nput_to_s3(from = \"data_processed/threatened.tif\", \n          to = \"Replication_wolf/data_processed/threatened.tif\")\n\n\n\n\nCode\n# Fetch outputs\n# Save outputs\nsave_from_s3(from = \"Replication_wolf/data_processed/non-threatened.tif\", \n             to = \"data_processed/non-threatened.tif\")\nsave_from_s3(from = \"Replication_wolf/data_processed/threatened.tif\", \n             to = \"data_processed/threatened.tif\")\n\n\n\n\n5.1.4 Join rasters\nThis steps corresponds to the file 004 - join_rasters.R, which original content can be examined below.\n\n\n\nCode\n#| echo: false\n#| warning: false\n#| class-output: \"sourceCode python\"\n\ncat(readLines(\"PA_matching_asis/004 - join_rasters.R\"), sep = \"\\n\")\n\n\n\nThis script cannot be run without modification. The location of the working directory was specific to the author machine, we replace by a generic location. The script fetches data from Curtis et al [INSERT CITATION], but the URL has changed. We replace with the new URL and modify too the file name with is now all in lowercase. The URLs for Curtis et al. data and WWF data have changed There is a typo on line 136: an object called PA_dists is called but no object exists with this name, however an object PAs_dist (without a final “s”) was created on the previous code line\n\n\nCode\njoin_script &lt;- \"PA_matching_asis/004 - join_rasters.R\"\nmy_wd &lt;- paste0(getwd(), \"/\")\n# Change the working directory location\nreplace_all(pattern = \"/home/chrisgraywolf/shared/analysis/PA_matching/\", \n            replacement = my_wd,\n            file = join_script)\n\nreplace_all(pattern = 'read_sf\\\\(\"data_input/PAs/WDPA_Jan2020-shapefile-polygons.shp\"\\\\)',\n            replacement = 'geoarrow::read_geoparquet_sf\\\\(\"data_input/PAs/WDPA_Jan2020_polygons.parquet\"\\\\)',\n            file = join_script)\n            \nreplace_all(pattern = 'PAs_tab\\\\$geometry = NULL',\n            replacement = 'PAs_tab = st_drop_geometry\\\\(PAs_tab\\\\)',\n            file = join_script)\n# Correct the typo in the code\nreplace_all(pattern = \"PAs_buffer = PA_dists &lt;= 10e3\",\n            replacement = \"PAs_buffer = PAs_dist &lt;= 10e3\",\n            file = join_script)\n\n\nGather data\n\n\nCode\n# Listing files in bucket\nmy_files &lt;- get_bucket_df(bucket = \"fbedecarrats\",\n                          prefix = \"Replication_wolf\",\n                          region = \"\")\n# Select PA shp\nmy_PAs &lt;- my_files %&gt;%\n  filter(str_detect(Key, \"/PAs/WDPA\")) %&gt;%\n  pluck(\"Key\")\n# Create paths in local machine\nmy_PAs_dest &lt;- my_PAs %&gt;%\n  str_remove(\"Replication_wolf/\")\n# Retrieve the data\nmap2(my_PAs, my_PAs_dest, save_from_s3)\n\n# Select WWF shp\nmy_biomes &lt;- my_files %&gt;%\n  filter(str_detect(Key, \"/official/\")) %&gt;%\n  pluck(\"Key\")\n# Create paths in local machine\nmy_biomes_dest &lt;- my_biomes %&gt;%\n   str_remove(\"Replication_wolf/\")\n# Retrieve the data\nmap2(my_biomes, my_biomes_dest, save_from_s3)\n\n\nRun script\n\n\nCode\njoin_script &lt;- \"PA_matching_asis/004 - join_rasters.R\"\nsource(join_script)\n\n\nSave outputs\n\n\nCode\n# Save outputs\n# List present tifs\n# See which are not already in S3\nprocessed_local &lt;- list.files(path = \"data_processed\",\n                              recursive = TRUE,\n                              full.names = TRUE) %&gt;%\n  str_subset(., \"vrt$\", negate = TRUE)\n# Listing files in bucket\nmy_files &lt;- get_bucket_df(bucket = \"fbedecarrats\",\n                          prefix = \"Replication_wolf\",\n                          region = \"\")\nprocessed_s3 &lt;- my_files %&gt;%\n  filter(str_detect(Key, \"/data_processed/\")) %&gt;%\n  mutate(path = str_remove(Key, \"Replication_wolf/\")) %&gt;%\n  pluck(\"path\")\nto_put_s3 &lt;- processed_local[!processed_local %in% processed_s3]\nto_put_s3\npath_in_s3 &lt;- paste0(\"Replication_wolf/\", to_put_s3)\n# Put them in S3\nmap2(to_put_s3, path_in_s3, put_to_s3)\n\n\n\n\nCode\n# Listing files in bucket\nmy_files &lt;- get_bucket_df(bucket = \"fbedecarrats\",\n                          prefix = \"Replication_wolf\",\n                          region = \"\") %&gt;%\n  filter(str_detect(Key, \"keep$\", negate = TRUE))\n\nprocessed_s3 &lt;- my_files %&gt;%\n  filter(str_detect(Key, \"data_processed\")) %&gt;%\n  mutate(path = str_remove(Key, \"Replication_wolf/\")) %&gt;%\n  pluck(\"path\")\nprocessed_local &lt;- list.files(path = \"data_processed\",\n                                   recursive = TRUE,\n                                   full.names = TRUE)\nto_save_local &lt;- processed_s3[!processed_s3 %in% processed_local]\nto_save_local\nif (length(to_save_local) &gt; 0) {\n  path_s3_to_local &lt;- paste0(\"Replication_wolf/\", to_save_local)\n  map2(path_s3_to_local, to_save_local, save_from_s3)\n}\n\n\nExecute the code\n\n\nCode\njoin_script &lt;- \"PA_matching_asis/004 - join_rasters.R\"\nsource(join_script)\n\n\nNote : Message bizarre suite à lignes 78-93\nERROR 4: : No such file or directory\nWarning 1: Can't open . Skipping it\nERROR 4: data_input/lossyear.vrt: No such file or directory\nWarning messages:\n1: In system(cmd, intern = TRUE) :\n  running command '\"/usr/bin/gdalbuildvrt\" \"data_input/lossyear.vrt\" \"\"' had status 1\n2: In system(cmd, intern = TRUE) :\n  running command '\"/usr/bin/gdalwarp\" -te -17367530.4451614 -7342769.86350132 17366469.5548386 7342230.13649868 -tr 1000 1000 -t_srs \"+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs\" -of \"VRT\" \"data_input/lossyear.vrt\" \"data_input/lossyear_proj.vrt\"' had status 2\n\n\n5.1.5 Match pixels on covariates\n\n\nCode\nmatching_script &lt;- \"PA_matching_asis/005 - covariate_matching.jl\"\n\n# Modify the location of the working directory\nmy_wd &lt;-getwd()\n\n# The way Julia has to write pathes on windows is really weird\nif (str_detect(str_to_lower(Sys.getenv(\"OS\")), \"windows\")) {\n  # Needs bunch of escape characters in the working directory\n  my_wd &lt;- str_replace_all(my_wd, \"\\\\/\", \"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n  # \\U and \\s mean something so these letters get chopped in the process\n  my_wd &lt;- str_replace_all(my_wd, \"ers\", \"Users\")\n  # Need also to specify the location of the specific tif\n  replace_all(pattern = \"data_processed\\\\/rasters\\\\/cover.tif\", \n            replacement = \"data_processed\\\\\\\\\\\\\\\\rasters\\\\\\\\\\\\\\\\cover.tif\",\n            file = matching_script)\n} \n\n# Change the working directory location\nreplace_all(pattern = \"/home/chrisgraywolf/shared/analysis/PA_matching/\", \n            replacement = my_wd,\n            file = matching_script)\n\n# Need to specify the output type (DataFrame) in CSV.read command \nreplace_all(pattern = \"CSV\\\\.read\\\\(\\\"data_processed/PAs_tab\\\\.csv\\\"\\\\)\", \n            replacement = \n              \"CSV\\\\.read\\\\(\\\"data_processed/PAs_tab\\\\.csv\\\", DataFrame\\\\)\",\n            file = matching_script)\n\n# No need to load this package: not used, and throws useless warnings\nreplace_all(pattern = \"using RCall\",\n            replacement = \"\",\n            file = matching_script)\n\n# Update syntax for joins: join(..., kind = inner) =&gt; innerjoin()\nreplace_all(pattern = \"out = join\\\\(data_treatment,\",\n            replacement = \"out = leftjoin\\\\(data_treatment,\",\n            file = matching_script)\nreplace_all(pattern = \"on = \\\\:PAs,\",\n            replacement = \"on = \\\\:PAs\\\\)\",\n            file = matching_script)\nreplace_all(pattern = \"kind = \\\\:left\\\\)\",\n            replacement = \"\",\n            file = matching_script)\n\n# Need to add a dot before equal of in-place operations\nreplace_all(pattern = \"out\\\\[\\\\:,var\\\\] = mean\\\\(df_small\\\\[\\\\:,var\\\\]\\\\)\",\n            replacement = \"out\\\\[\\\\:,var\\\\] .= mean\\\\(df_small\\\\[\\\\:,var\\\\]\\\\)\",\n            file = matching_script)\n\n# Update joining syntax\nreplace_all(pattern = \"join(data_treatment_coarsened[!,vcat\\\\(:PAs,:STATUS_YR,match\\\\)], data_control_coarsened, on = match, kind = :inner)\",\n            replacement = \"innerjoin(data_treatment_coarsened[!, vcat\\\\(:PAs, :STATUS_YR, match\\\\)], data_control_coarsened, on = match\\\\)\",\n            file = matching_script)\n\n# On line 141, update inner join syntax\nreplace_all(pattern = \"join\\\\(data_treatment_coarsened\",\n            replacement = \"innerjoin\\\\(data_treatment_coarsened\",\n            file = matching_script)\nreplace_all(pattern = \", kind = \\\\:inner\\\\)\",\n            replacement = \"\\\\)\",\n            file = matching_script)\n\n# Update the by() syntax =&gt; combine(groupby())\nreplace_all(pattern = 'by\\\\(df, setdiff\\\\(names\\\\(df\\\\), \\\\[\"treatment\"\\\\]\\\\)\\\\)',\n            replacement = 'combine\\\\(groupby\\\\(df, setdiff\\\\(names\\\\(df\\\\), \\\\[\"treatment\"\\\\]\\\\)\\\\)\\\\)',\n            file = matching_script)\nreplace_all(pattern = \"by\\\\(data_treatment, \\\\:PAs\\\\)\",\n            replacement = \"combine\\\\(groupby\\\\(data_treatment, \\\\:PAs\\\\)\\\\)\",\n            file = matching_script)\nreplace_all(pattern = \"by\\\\(data_matched, \\\\:PAs\\\\)\",\n            replacement = \"combine\\\\(groupby\\\\(data_matched, \\\\:PAs\\\\)\\\\)\",\n            file = matching_script)\n\n# Adjust DataFrame syntax and dot in front of equal for in-place operation\nreplace_all(pattern = \"\\\\(df\\\\[n\\\\] = NaN\\\\)\",\n            replacement = \"\\\\(df\\\\[\\\\:, n\\\\] .= NaN\\\\)\",\n            file = matching_script)\n\n# This also has to be updated to follow DataFrame syntax\nreplace_all(pattern = \"mean\\\\(convert\\\\(Array,df_small\\\\[\\\\:,cont_vars\\\\]\\\\), dims=1\\\\)\\\\[1,\\\\:\\\\]\",\n            replacement = \"\\\\[mean\\\\(col\\\\) for col = eachcol\\\\(df_small\\\\[\\\\:,cont_vars\\\\]\\\\), row=1\\\\]\",\n            file = matching_script)\n\n# Uncomment the lines launching the generation of the main results\nreplace_all(pattern = \"#data_matched\",\n            replacement = \"data_matched\",\n            file = matching_script)\nreplace_all(pattern = \"#CSV.write\",\n            replacement = \"CSV.write\",\n            file = matching_script)\n\n\nn_cores &lt;- parallel::detectCores()\nreplace_all(pattern = \"addprocs\\\\(12\\\\)\",\n            replacement = paste0(\"addprocs\\\\(\", n_cores - 1, \"\\\\)\"),\n            file = matching_script)\n\n\n\n\nCode\nlibrary(JuliaCall)\njulia_source(matching_script)\n\n\n\n\nCode\n# Save outputs\n# List present tifs\n# See which are not already in S3\nprocessed_local &lt;- list.files(path = \"data_processed\",\n                              recursive = TRUE,\n                              full.names = TRUE) %&gt;%\n  str_subset(., \"vrt$\", negate = TRUE)\n# Listing files in bucket\nmy_files &lt;- get_bucket_df(bucket = \"fbedecarrats\",\n                          prefix = \"Replication_wolf\",\n                          region = \"\")\nprocessed_s3 &lt;- my_files %&gt;%\n  filter(str_detect(Key, \"/data_processed/\")) %&gt;%\n  mutate(path = str_remove(Key, \"Replication_wolf/\")) %&gt;%\n  pluck(\"path\")\nto_put_s3 &lt;- processed_local[!processed_local %in% processed_s3]\nto_put_s3\npath_in_s3 &lt;- paste0(\"Replication_wolf/\", to_put_s3)\n# Put them in S3\nmap2(to_put_s3, path_in_s3, put_to_s3)\nput_to_s3(\"data_processed/PA_df.RDS\", \"Replication_wolf/data_processed/PA_df.RDS\")\n\n\n\n\n5.1.6 Merge the results\n\n\nCode\njoin_results_script &lt;- \"PA_matching_asis/006 - join_results.R\"\n# Do not change working directory\nreplace_all(pattern = \"setwd\\\\(project_dir\\\\)\",\n            replacement = \"# setwd\\\\(project_dir\\\\)\",\n            file = join_results_script)\nreplace_all(pattern = 'read_sf\\\\(\"data_input/PAs/WDPA_Jan2020-shapefile-polygons.shp\"\\\\)',\n            replacement = 'geoarrow::read_geoparquet_sf\\\\(\"data_input/PAs/WDPA_Jan2020_polygons.parquet\"\\\\)',\n            file = join_results_script)\nreplace_all(pattern = 'wb\\\\(indicator=\"NY.GDP.PCAP.KD\", startdate=2000, enddate=2018\\\\)',\n            replacement = 'read_csv\\\\(\"GDP_PC_PPP2010_2000-2018.csv\"\\\\)',\n            file = join_results_script)\n# Legacy regions in country_codes: https://vincentarelbundock.github.io/countrycode/reference/codelist.html\nreplace_all(pattern = '\"region\"',\n            replacement = '\"region23\"',\n            file = join_results_script)\n            \nreplace_all(pattern = \"/home/chrisgraywolf/shared/analysis/PA_loss/data/\",\n            replacement = \"PA_matching_asis/\",\n            file = join_results_script)\n\n\n\n\nCode\nsource(join_results_script)\n\n\n\n\n5.1.7 Compute SVC model\n\n\nCode\nsvc_model_script &lt;- \"PA_matching_asis/007 - SVC model.R\"\nreplace_all(pattern = \"select\",\n            replacement = \"dplyr::select\",\n            file = svc_model_script)\nreplace_all(pattern = '\\\\\"main\\\\\"',\n            replacement = '\\\\\"change\\\\\"',\n            file = svc_model_script)\n\n\n\n\nCode\nsource(svc_model_script)\n\n\n\n\nCode\n# Save outputs\n# List present tifs\n# See which are not already in S3\nprocessed_local &lt;- list.files(path = \"output\",\n                              recursive = TRUE,\n                              full.names = TRUE) %&gt;%\n  str_subset(., \"vrt$\", negate = TRUE)\n# Listing files in bucket\nmy_files &lt;- get_bucket_df(bucket = \"fbedecarrats\",\n                          prefix = \"Replication_wolf\",\n                          region = \"\")\nprocessed_s3 &lt;- my_files %&gt;%\n  filter(str_detect(Key, \"/output/\")) %&gt;%\n  mutate(path = str_remove(Key, \"Replication_wolf/\")) %&gt;%\n  pluck(\"path\")\nto_put_s3 &lt;- processed_local[!processed_local %in% processed_s3]\nto_put_s3\npath_in_s3 &lt;- paste0(\"Replication_wolf/\", to_put_s3)\n# Put them in S3\nmap2(to_put_s3, path_in_s3, put_to_s3)\n\n\n\n\n5.1.8 Perform country level analysis\n\n\nCode\ncountry_analysis_script &lt;- \"PA_matching_asis/007 - country level analysis.R\"\n# ne_load fetches from the temporary location (which is lost if we run in \n# different threads with `source(script)`). This ensures the data is fetched\nreplace_all(pattern = \"ne_load\",\n            replacement = \"ne_download\",\n            file = country_analysis_script)\nreplace_all(pattern = 'read.dbf\\\\(\"data_input/PAs/WDPA_Jan2020-shapefile-polygons.dbf\", as.is=TRUE\\\\)',\n            replacement = 'arrow::read_parquet\\\\(\"data_input/PAs/WDPA_Jan2020_polygons.parquet\", col_select = -Shape\\\\)',\n            file = country_analysis_script)\n\n\n\n\n5.1.9 Produce summary figures\n\n\nCode\nsummary_figures_script &lt;- \"PA_matching_asis/007 - summary figures.R\"\n\nreplace_all(pattern = 'read_sf\\\\(\"data_input/PAs/WDPA_Jan2020-shapefile-polygons.shp\"\\\\)',\n            replacement = 'geoarrow::read_geoparquet_sf\\\\(\"data_input/PAs/WDPA_Jan2020_polygons.parquet\"\\\\)',\n            file = summary_figures_script)\n\nreplace_all(pattern = \"ne_load\",\n            replacement = \"ne_download\",\n            file = summary_figures_script)\n\nreplace_all(pattern = \"select\",\n            replacement = \"dplyr::select\",\n            file = summary_figures_script)\n\nreplace_all(pattern = '54030',\n            replacement = '\"ESRI:54030\"',\n            file = summary_figures_script)\n\n\n\n\n5.1.10 Results\nIn this section, we reproduce a series of outputs generated in the code of the script 007 - summary figures.R that correspond to the main results reported in the original study. We find moderate differences in descriptive statistics and negligible ones on the outcomes on conservations.\n\n5.1.10.1 Conservation extent\nThe original study reports &gt; “Globally, 15.7% of forest is formally protected”\n\n\nCode\ntab &lt;- table(cover[] &gt;= 30, PA_rast[], useNA=\"always\")\nprop.table(tab,1) # 0.18650047\n\n\nWe find that 18.7% of the forest is formally protected. This raises the questions about whether the authors of the original study used the same version of WDPA or biome extent than the ones we had access to for the replication, although we fetched it from the same sources. We will try to better under\n\n\n5.1.10.2 PA characteristics\nThe original study reports: &gt; “Prior to matching with control areas, our primary PA dataset contained 25,348 PAs. However, 7,177 (28.3%) of these PAs could not be matched with any unprotected pixels having similar matching-covariate values. Consequently, after applying coarsened exact matching to identify control areas, our final dataset contained 18,171 PAs, with a total area of 5,293,217 km2 (Fig. 1 and Extended Data Fig. 1).” &gt; “In absolute terms, the 18,171 PAs in our analysis had an average annual forest loss rate of ~1.53 Mha.”\n\n\nCode\nnrow(readRDS(\"data_processed/PA_df.RDS\") %&gt;% dplyr::filter(group == \"main\"))\naddmargins(table(matched=df$matched, cat=df$cat_simple, gp=df$group))\n\n\nIn our replication, we have in the primary PA dataset we find 25850 Protected areas fitting the analysis criteria defined by the source code. Of those 18,728 could be matched protected area, covering 6,167,491 km2. Our replication finds ~1.71 Mha of deforestatoin in the 18,728 analyzed.\n\n\nCode\naverage_initial &lt;- 5293217 / 18171 # 291.3003\nadditional_PAs &lt;- 18728 - 18171 # 557\nadditional_area &lt;- 6167491 - 5293217 # 874274\naverage_additional &lt;- additional_area / additional_PAs # 1569.612\n\n\nThis suggest that we did not get exactly the same WDPA dataset from the beginning, although the one we used (WDPA_Jan2020_Public) is the one referred to in the source code (WDPA_Jan2020-shapefile-polygons.shp), and mentionned in the bibliography of the original study (reference 59 is: “The World Database on Protected Areas (WDPA) (IUCN and UNEP-WCMC, accessed 1 January 2020)”). We have 557 additional PAs in our replication, corresponding to 874,274 km2. According to the relative number and areas, we deduce that these 557 additionnal PAs have an average are of 1569 km2, compared to an average of 291 km2 for the PAs reported. We conclude that the additional PAs tend to be larger than the ones used in the original dataset, but we are not able to identify them as the data used for the original study have not been published.\n\n\n5.1.10.3 PAs without loss\nThe original study reports: &gt; In our analysis, 28.7% of the PAs did not have any forest loss. Among PAs with known management category, deforestation rates were highest in nonstrict PAs in Africa (0.31% per year), Europe (0.29% per year) and South America (0.19% per year), and lowest in strict PAs in Oceania (0.02% per year)\n\n\nCode\nmean(df$PA_loss ==  0)\n\n\nIn our replication, we find that 27.9% of of the PAs did not have any forest loss.\n\n\nCode\nn_no_loss_replic &lt;- round(18728*0.279) # 5225.112\nn_no_loss_origin &lt;- round(18171*0.287) # 5215.077\nn_no_loss_additionnal &lt;- n_no_loss_replic - n_no_loss_origin\nn_no_loss_additionnal / additional_PAs\n\n\nIn absolute terms, this means that the original study found that 5215 PAs had no forest loss, which corresponds to a 28.7% of 18,171 PAs, while the replication finds 5225 PAs without forest loss (28.7% of PAs with no forest loss), which corresponds to a 27.9% of 18,728 PAs. This means that, of the 557 additionnal PAs included in the replication, only 10 had no forest loss (1.8%).\nWe can also reproduce figure 2: \n\n\n5.1.10.4 Conservation impact: before 2001\nThe original study reports in its abstract: &gt; Overall, protected areas did not eliminate deforestation, but reduced deforestation rates by 41%.\nAnd in its results section:` &gt; Overall, PAs reduced, but did not eliminate, deforestation; the median annual deforestation rate in control areas (0.54%; s.d. = 2.21%) was 4.97 times higher than within PAs (0.11% per year; s.d. = 2.45%) (Fig. 2). In absolute terms, the 18,171 PAs in our analysis had an average annual forest loss rate of ~1.53 Mha. In our analysis, 28.7% of the PAs did not have any forest loss. Among PAs with known management category, deforestation rates were highest in nonstrict PAs in Africa (0.31% per year), Europe (0.29% per year) and South America (0.19% per year), and lowest in strict PAs in Oceania (0.02% per year)\n\n\nCode\ndiff_mean &lt;- round(100*(1-mean(inside)/mean(outside)),3) # 42.74% instead of 41.12% \ndiff_add_median &lt;- round(100*median(outside-inside),3) # median additive difference of 0.226% vs 0.194%\niqr_diff &lt;- IQR(outside-inside) # 0.008525058\nout_median &lt;- median(outside) # 0.005685758 -&gt; 0.57% instead of 0.54%\nout_sd &lt;- sd(outside) # 0.02531807 instead of 2.21%\nin_median &lt;- median(inside) # 0.00116665 almost identical to 0.11% per year\nin_ds &lt;- sd(inside) # 0.02413182 vs. 2.45%\ndiff_median &lt;- median(outside)/median(inside) # 4.873577\nannual_area_deforest &lt;- (100*sum(df$GIS_AREA * df$cover_loss/100)/18)/1e6 # 1.71)/ instead of 1.53\ndefor_by_status_continent &lt;- tab[order(tab$value, decreasing=TRUE),]\n# Africa Nonstrict 0.30544782\n# Europe Nonstrict 0.24789509\n# South America Nonstrict 0.20705785\n# ...\n# Oceania    Strict 0.03195096\n# Oceania Nonstrict 0.03166633\n\n\nThe results of the comparisons are very close in relative terms. In absolute term the difference is 11.8% higher: 1.71 Mha in annual loss instead of 1.53.\nWe reproduce the Figure 3:\n\n\n\nFig. 3: Forest loss in and around PAs\n\n\n\n\n5.1.10.5 Conservation impact 2002-2017\nThe original study reports: &gt; We identified 9,875 PAs established between 2002 and 2017 that were suitable for inclusion in our spatiotemporal analysis. The establishment of these PAs was associated with a moderate increase in the deforestation rate (average = 0.19%; s.e.m. = 0.02%), whereas control areas saw a larger increase in the deforestation rate (average = 0.61%; s.e.m. = 0.02%) over the same time span (Extended Data Fig. 3). This overall pattern was observed for both strict and nonstrict PAs in lower and higher GDP countries (Extended Data Fig. 3).\n\n\nCode\ntreat_loss_before &lt;- round(100*mean(df$PA_loss_before), 3) # 0.431\ntreat_loss_after &lt;- round(100*mean(df$PA_loss_after), 3) # 0.477\nP = df$PA_loss_after - df$PA_loss_before\nC = df$Control_loss_after - df$Control_loss_before\nreduc_treat &lt;- round(100*mean(P), 3)\nreduc_cont &lt;- round(100*mean(C), 3)\nreduc_treat_sd 100*sd(P)/sqrt(length(P))\nreduc_cont_sd &lt;-  100*sd(C)/sqrt(length(C))\ncont_loss_brefore &lt;- round(100*mean(df$Control_loss_before), 3) # 0.751\ncont_loss_after &lt;- round(100*mean(df$Control_loss_after), 3) # 1.032\ncontin_diff &lt;- round(100*tapply(df_small$value, list(df_small$variable, df_small$Continent), mean),3)\n#                 South America Oceania North America Europe  Asia Africa\n# Control areas           0.084   0.065        -0.014  0.597 0.273  0.818\n# Protected areas         0.133   0.214        -0.125  0.154 0.130  0.293\n\n\nCheck difference of averages before! Reported ratios amount to DiD, but initial levels cast doubts on parallel trends, etc.\n\n\n\n5.1.11 Run matching sensitivity\n\n\nCode\nreplace_all(pattern = \"\",\n            replacement = \"\",\n            file = join_results_script)"
  },
  {
    "objectID": "replication_report/06_reimplement_matching.html",
    "href": "replication_report/06_reimplement_matching.html",
    "title": "6  Reimplement matching",
    "section": "",
    "text": "7 Re-implementing the logic of original study with a new code\nWe have not yet been able to successfully replicate the original study as their code does not run successfully and generates errors. We identified the origin of some errors and corrected them, but we remained stuck with Julia errors. We tried different computing environment : local, cloud servers and with several different configurations (Windows, Linux, different R, python and Julia versions, etc.), and with large amounts of CPUs and memory.\nWe believe that the computing sequence elaborated by this study authors is particularly complex because of the very large amount of data to be processed. We propose a different approach that seems more straight forward and use the same entry parameters as the original study.\nHere we attempt to reproduce every analysis steps and parameters of the original study but to implement a different data processing strategy that:\nInstead of downloading and processing all the GFC data like Wolf and al., we instead keep only the data located within the perimeter of the biomes of interest.\nWolf et al. downloaded each information source in a separate raster, that they later on joined. We find easier and less error-prone to stack all the information as bands of the same raster. The data might be downloaded in separate files for different national segment of each biome, but for the same regions, all the complementary information is attached to the same pixels with the same resolution (alias, a “clean” table).\nThe previous code creates for each area of interest a raster with the following bands: cover, loss, lossyear, elev, slope, travel_time, pop_dens, PA, WDPA ID number, PA_status_yr, PA_buffer, country, biome.\nNow we will iterate to download this raster in separated file, with one file per national segment of each biome of interest.\nWe can visualize one of the country data, for instance in Madagascar:"
  },
  {
    "objectID": "replication_report/06_reimplement_matching.html#fetch-and-prepare-data-from-other-sources-than-gee",
    "href": "replication_report/06_reimplement_matching.html#fetch-and-prepare-data-from-other-sources-than-gee",
    "title": "6  Reimplement matching",
    "section": "7.1 Fetch and prepare data from other sources than GEE",
    "text": "7.1 Fetch and prepare data from other sources than GEE\nWe gather data for national boundaries and biomes and combine it.\n\n\nCode\nif (file.exists(\"revamp/countries/countries.rds\")) {\n  countries <- read_rds(\"revamp/countries/countries.rds\")\n} else {\n  # Countries  using geodata (ie. GADM), rnaturalearth is broken\n  dir.create(\"revamp\")\n  dir.create(\"revamp/countries\")\n  \n  # This data is from GADM which seems more complete and accurate\n  countries <- world(resolution=2, level=0, path = \"revamp/countries\")\n  countries <- countries %>%\n    st_as_sf() %>%\n    st_make_valid() %>%\n    mutate(country_num = 1:nrow(.), .before = GID_0)\n  # Fetch data from rnaturalearth for continents\n  countries2 <- ne_download(scale = 10,\n                            type = \"countries\",\n                            category = \"cultural\",\n                            destdir = \"revamp/countries\",\n                            load = TRUE,\n                            returnclass = \"sf\")\n  country_continent <- countries2 %>%\n    st_drop_geometry() %>%\n    select(ADM0_A3, continent = CONTINENT)\n  countries <- countries %>%\n    left_join(country_continent, join_by(GID_0 == ADM0_A3)) %>%\n    mutate(continent = case_when(\n      NAME_0 == \"Åland\" ~ \"Europe\", # some territories are missing\n      NAME_0 == \"Bonaire, Saint Eustatius and Saba\" ~ \"North America\",\n      NAME_0 == \"Caspian Sea\" ~ \"Europe\",\n      NAME_0 == \"Christmas Island\" ~ \"Oceania\",\n      NAME_0 == \"French Guiana\" ~ \"South America\",\n      NAME_0 == \"Guadeloupe\" ~ \"North America\",\n      NAME_0 == \"Kosovo\" ~ \"Europe\",\n      NAME_0 == \"Martinique\" ~ \"North America\",\n      NAME_0 == \"Mayotte\" ~ \"Africa\",\n      NAME_0 == \"Paracel Islands\" ~ \"Asia\",\n      NAME_0 == \"Réunion\" ~ \"Africa\",\n      NAME_0 == \"South Sudan\" ~ \"Africa\",\n      .default = continent),\n      continent = ifelse(continent == \"Seven seas (open ocean)\",\n                         \"Open ocean\", continent))\n  write_rds(countries, \"revamp/countries/countries.rds\")\n}\n\n# We prepare ecoregion polygons (only at first run, if not already done)\nif (!file.exists(\"revamp/aois/aois.gpkg\")) {\n  # Fetch biomes\n  dir.create(\"revamp/biomes\")\n  if (!file.exists(\"revamp/biomes/wwf_biomes.zip\")) {\n    wwf_url <- paste0(\"https://files.worldwildlife.org/wwfcmsprod/files/\", \n                      \"Publication/file/6kcchn7e3u_official_teow.zip\")\n    download.file(wwf_url, \"revamp/biomes/wwf_biomes.zip\")\n    unzip(\"revamp/biomes/wwf_biomes.zip\", exdir = \"revamp/biomes\")\n  }\n  # Merge ecoregions with the same biome\n  biomes <- st_read(\"revamp/biomes/official/wwf_terr_ecos.shp\") %>%\n    st_make_valid() %>%\n    group_by(BIOME) %>%\n    summarise()\n# Rename according to documentation and filter\n  biomes <- biomes %>%\n    rename(biome_num = BIOME) %>%\n    mutate(biome_name = case_when(\n      biome_num == 1 ~ \"Tropical & Subtropical Moist Broadleaf Forests\",\n      biome_num == 2 ~ \"Tropical & Subtropical Dry Broadleaf Forests\",\n      biome_num == 3 ~ \"Tropical & Subtropical Coniferous Forests\",\n      biome_num == 4 ~ \"Temperate Broadleaf & Mixed Forests\",\n      biome_num == 5 ~ \"Temperate Conifer Forests\",\n      biome_num == 6 ~ \"Boreal Forests/Taiga\",\n      biome_num == 7 ~ \"Tropical & Subtropical Grasslands, Savannas & Shrublands\",\n      biome_num == 8 ~ \"Temperate Grasslands, Savannas & Shrublands\",\n      biome_num == 9 ~ \"Flooded Grasslands & Savannas\",\n      biome_num == 10 ~ \"Montane Grasslands & Shrublands\",\n      biome_num == 11 ~ \"Tundra\",\n      biome_num == 12 ~ \"Mediterranean Forests, Woodlands & Scrub\",\n      biome_num == 13 ~ \"Deserts & Xeric Shrublands\",\n      biome_num == 14 ~ \"Mangroves\",\n      .default = \"Unknown\"), .before = geometry)\n  my_biomes <- biomes %>%\n    filter(biome_name %in% c(\"Tropical & Subtropical Moist Broadleaf Forests\",\n                             \"Tropical & Subtropical Dry Broadleaf Forests\",\n                             \"Tropical & Subtropical Coniferous Forests\",\n                             \"Temperate Broadleaf & Mixed Forests\",\n                             \"Temperate Conifer Forests\",\n                             \"Boreal Forests/Taiga\",\n                             \"Mangroves\")) %>%\n    mutate(biome_num = case_when(\n      biome_name == \"Tropical & Subtropical Moist Broadleaf Forests\" ~ 1,\n      biome_name == \"Tropical & Subtropical Dry Broadleaf Forests\" ~ 2,\n      biome_name == \"Tropical & Subtropical Coniferous Forests\" ~ 3,\n      biome_name == \"Temperate Broadleaf & Mixed Forests\" ~ 4,\n      biome_name == \"Temperate Conifer Forests\" ~ 5,\n      biome_name == \"Boreal Forests/Taiga\" ~ 6,\n      biome_name == \"Mangroves\" ~ 7))\n  \n  # Areas of interest (AOIs) combine countries and biomes\n  aois <- st_intersection(countries, my_biomes) %>%\n    mutate(area = st_area(.)) %>%\n    arrange(area) # to start with the smallest for testing\n  # save as geoparquet\n  st_write(aois, \"revamp/aois/aois.gpkg\") \n} else {\n  aois <- st_read(\"revamp/aois/aois.gpkg\")\n}\n\n\nReading layer `aois' from data source \n  `C:\\Users\\fbede\\Documents\\Statistiques\\PA_matching\\replication_report\\revamp\\aois\\aois.gpkg' \n  using driver `GPKG'\nSimple feature collection with 364 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.9994 ymin: -55.97751 xmax: 179.9992 ymax: 71.41746\nCRS:           unknown\n\n\nCode\naois %>%\n  rename(Biome = biome_name) %>%\n  tm_shape() +\n  tm_polygons(col = \"Biome\") + \n  tm_borders() + \n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"bottom\")\n\n\n\n\n\nAreas of interest (biome segmented along national boundaries)\n\n\n\n\nWe obtain 364 polygons of country/biomes combinations.\nWe also download the data from (Curtis et al. 2018) on deforestation drivers, which is a raster data.\n\n\nCode\n# We only execute this once\nif (!file.exists(\"revamp/drivers/curtis_et_al_orig.tif\")) {\n  science_url <- paste0(\"https://www.science.org/action/downloadSupplement?\",\n                        \"doi=10.1126%2Fscience.aau3445&file=aau3445-data-s3.tif\")\n  download.file(url = science_url, \n                destfile = \"revamp/drivers/curtis_et_al_orig.tif\",\n                method = \"curl\")\n  # load country + biome data\ndrivers <- rast(\"revamp/drivers/curtis_et_al_orig.tif\") %>%\n  project(\"epsg:4326\") %>%\n  writeRaster(\"revamp/drivers/drivers_curtis.tif\")\n}"
  },
  {
    "objectID": "replication_report/06_reimplement_matching.html#move-external-data-to-gee-and-prepare-all-information-there",
    "href": "replication_report/06_reimplement_matching.html#move-external-data-to-gee-and-prepare-all-information-there",
    "title": "6  Reimplement matching",
    "section": "7.2 Move external data to GEE and prepare all information there",
    "text": "7.2 Move external data to GEE and prepare all information there\nThe first step of the data analysis workflow of Wolf et al. consisted in fetching data from Google Earth Engine. They downloaded each dataset (forest cover, forest loss, forest gain, elevation, slope, travel time and population density) as separate raster files to process them locally on their own computer. Because of the massive amount of such data, the subsequent steps in their analysis workflow are complex, rely from different languages, a large number of libraries (several of them now deprectated), fails to run and we are unable to debug it.\nThe Google Earth Engine can be leveraged to do much more than only downloading data, and therefore we plan to re-implement on this cloud environment the data preparation steps, instead of doing it locally on individual machines.\nWe use the R package rgee as an interface to Google Earth engine API (Aybar et al. 2020).\n\n\nCode\n# Sometime rgee does not find the right python env, so we specify it\nreticulate::use_python(\"C:/Users/fbede/AppData/Local/r-miniconda/envs/rgee\")\nlibrary(rgee) # accesses GEE through its python API\nlibrary(rgeeExtra)\nee_Initialize(user = \"ndef\", drive = TRUE, )\ndir.create(\"revamp/gee\")\n\n\nWe start by uploading the country, biome and deforestation drivers’ data prepared in the previous step.\n\n\nCode\n# We add this manually, as we had issues with API auth. protocol for GSC\ndrivers_curtis <- ee$Image(\"projects/ee-fbedecarrats/assets/drivers_curtis\")\n# We also load the AOIs manually due to the same problem\nif (!file.exists(\"revamp/aois/aois_shp.zip\")) {\n  dir.create(\"revamp/aois\")\n  st_write(aois, \"revamp/aois/aois.shp\")\n  aoi_files = list.files(path = \"revamp/aois\", pattern = \"aois.*\", \n                         full.names = TRUE)\n  zip(zipfile = \"revamp/aois/aois_shp.zip\", files = aoi_files)\n}\naois_ee <- ee$FeatureCollection(\"projects/ee-fbedecarrats/assets/aois\")\n# We get the colors used by R using tmaptools::palette_explorer()\nbiome_fills <- ee$Image()$byte()$\n  paint(featureCollection = aois_ee,\n        color = \"biome_num\")\nbiome_nation_borders <- ee$Image()$byte()$\n  paint(featureCollection = aois_ee,\n        color = 1,\n        width = 1)\n\nr_map_palette <- c(\"b3de69\", # light green: Tropical & Subtropical Moist Broadleaf Forests \n                   \"fdb462\", # light orange: Tropical & Subtropical Dry Broadleaf Forests\n                   \"80b1d3\", # blue: Tropical & Subtropical Coniferous Forest\n                   \"bebada\", # purple: Temperate Broadleaf & Mixed Forests\n                   \"fb8072\", # dark orange: Temperate Conifer Forests\n                   \"8dd3c7\", # turquois: Boreal Forests/Taiga\n                   \"ffffb3\") # yellow: Mangroves\nMap$addLayer(biome_fills, list(min = 1, max = 7, palette = r_map_palette)) +\n  Map$addLayer(biome_nation_borders, list(palette = \"000000\"))\n\n\n\n\n\nDiplay of biomes segmented by national boarders on google earth engine"
  },
  {
    "objectID": "replication_report/07_observations_initial_study.html",
    "href": "replication_report/07_observations_initial_study.html",
    "title": "7  Discussion of processing choices",
    "section": "",
    "text": "8 Discussion of possible issues with the original study\nHere we list a series of data processings that raised questions during the re-implementation."
  },
  {
    "objectID": "replication_report/07_observations_initial_study.html#year-of-deforestation-corresponds-to-mode-for-coarsened-polygon",
    "href": "replication_report/07_observations_initial_study.html#year-of-deforestation-corresponds-to-mode-for-coarsened-polygon",
    "title": "7  Discussion of processing choices",
    "section": "8.1 Year of deforestation corresponds to mode for coarsened polygon",
    "text": "8.1 Year of deforestation corresponds to mode for coarsened polygon\n\n8.1.1 Difference in computation\nThe authors describe the corresponding part of their data preparation as “We used the following layers from version 1.6 of Hansen et al.2: tree cover in the year 2000 (percentage of each pixel); forest loss between 2001 and 2018 (binary raster) forest gain between 2000 and 2012 (also binary); and primary year associated with forest loss event” (Wolf et al. 2021, suppl. mat. p. 2). In fact, the way this pimary year associated with forest loss event is computationnaly processed as follow.\n\n\nCode\n,\nlossyear = lossyear.updateMask(lossyear.neq(0)).reduceResolution(reducer=ee.Reducer.mode(maxRaw=20), bestEffort=True)\n\n\nThis means that while aggregating several (~1000) pixels, the authors retained only one year, corresponding to the year in which the most of occurrence of deforestation occurred among all the aggregated polygons.\nThis might alter the calculation, potentially masking the first years of deforestation and therefore delaying the impact of conservation (ie. deforestation that started before PA creation only appears after PA creation.\n\n\n8.1.2 Incidence on results\nWe try to follow the same data process, but changing this by calculating forest loss event for each year.\n[to be done]"
  },
  {
    "objectID": "replication_report/07_observations_initial_study.html#coastal-protected-areas-are-removed",
    "href": "replication_report/07_observations_initial_study.html#coastal-protected-areas-are-removed",
    "title": "7  Discussion of processing choices",
    "section": "8.2 Coastal protected areas are removed",
    "text": "8.2 Coastal protected areas are removed\n\n8.2.1 Issue in computation\nWolf et al. (2021, suppl. mat. p.2) indicate that they excluded PAs that were exclusiverly marine, but this operation was coded as follows.\n\n\nCode\nmain_PAs = PA_df[(PA_df.MARINE .== 0) .& (PA_df.STATUS .!= \"Proposed\") .& (PA_df.GIS_AREA .>= 1),:]\n\n\nThe MARINE variable in WDPA is coded as follows:\n\n“0 (predominantly or entirely terrestrial), 1 (Coastal: marine and terrestrial), and 2 (predominantly or entirely marine). The value ‘1’ is only used for polygons.” (WDPA Manual version 1.6, 2019).\n\nThis means that in practice, coastal areas have been excluded from the analysis. This can be problematic, considering for instance that coastal areas can expand far inland, and that they include among other biomes the mangroves, that were one of the focus biome for the analysis.\n\n\n8.2.2 Incidence on results\nWe try to compute the same analysis, but including the areas that were both terrestial or coastal."
  },
  {
    "objectID": "replication_report/07_observations_initial_study.html#reprojection-after-slope-calculation-and-resolution-reduction",
    "href": "replication_report/07_observations_initial_study.html#reprojection-after-slope-calculation-and-resolution-reduction",
    "title": "7  Discussion of processing choices",
    "section": "8.3 Reprojection after slope calculation and resolution reduction?",
    "text": "8.3 Reprojection after slope calculation and resolution reduction?\nThe Google earth engine documentation warns against reprojecting after slope calculation or resolution reduction. It could be due to some internal processing contraints but my understanding is that it is a contraint from the area calculation."
  },
  {
    "objectID": "replication_report/references.html",
    "href": "replication_report/references.html",
    "title": "References",
    "section": "",
    "text": "Aybar, Cesar, Qiusheng Wu, Lesly Bautista, Roy Yali, and Antony Barja.\n2020. “Rgee: An r Package for Interacting with Google Earth\nEngine.” Journal of Open Source Software 5 (51): 2272.\n\n\nBédécarrats, Florent, Marc Bouvier, Jeanne de Montalembert, and Marin\nFerry. 2022. “Aires Protégées.” In, IRD. Tutoriel de\nFormation. Tuléar. https://fbedecarrats.github.io/conservation-deforestation-madagascar/01-aires_protegees.html.\n\n\nCurtis, Philip G., Christy M. Slay, Nancy L. Harris, Alexandra\nTyukavina, and Matthew C. Hansen. 2018. “Classifying Drivers of\nGlobal Forest Loss.” Science 361 (6407): 11081111.\n\n\nVitolo, Claudia, Yehia Elkhatib, Dominik Reusser, Christopher J. A.\nMacleod, and Wouter Buytaert. 2015. “Web Technologies for\nEnvironmental Big Data.” Environmental Modelling &\nSoftware 63 (January): 185–98. https://doi.org/10.1016/j.envsoft.2014.10.007.\n\n\nWolf, Christopher, Taal Levi, William J. Ripple, Diego A.\nZárrate-Charry, and Matthew G. Betts. 2021. “A Forest Loss Report\nCard for the World’s Protected Areas.” Nature\nEcology & Evolution 5 (4): 520–29. https://doi.org/10.1038/s41559-021-01389-0."
  }
]